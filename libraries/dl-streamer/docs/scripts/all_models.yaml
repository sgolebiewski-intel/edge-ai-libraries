# Newest models
clip-vit-base-patch32:
  source: public
  format: pytorch
  task_type: lvm
  readme: https://huggingface.co/openai/clip-vit-base-patch32
  source_readme: https://huggingface.co/openai/clip-vit-base-patch32
  labels-file:
  model-proc: not required
clip-vit-base-patch16:
  source: public
  format: pytorch
  task_type: lvm
  readme: https://huggingface.co/openai/clip-vit-base-patch16
  source_readme: https://huggingface.co/openai/clip-vit-base-patch16
  labels-file:
  model-proc: not required
clip-vit-large-patch14:
  source: public
  format: pytorch
  task_type: lvm
  readme: https://huggingface.co/openai/clip-vit-large-patch14
  source_readme: https://huggingface.co/openai/clip-vit-large-patch14
  labels-file:
  model-proc: not required
YOLOv5:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/ultralytics/yolov5/releases/tag/v7.0
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v5.json
  files:
    - source: https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt
YOLOv7:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/WongKinYiu/yolov7
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v7.json
YOLOv8:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/ultralytics/ultralytics
  model-proc: not required
YOLOv8-OBB:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://docs.ultralytics.com/tasks/obb/
  model-proc: not required
YOLOv8-SEG:
  source: public
  format: pytorch
  task_type: instance segmentation
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://docs.ultralytics.com/tasks/segment/
  model-proc: not required
yolov8_license_plate_detector:
  source: public
  format: pytorch
  task_type: detection
  readme: https://github.com/Muhammad-Zeerak-Khan/Automatic-License-Plate-Recognition-using-YOLOv8
  source_readme: https://github.com/Muhammad-Zeerak-Khan/Automatic-License-Plate-Recognition-using-YOLOv8
  model-proc: not required
YOLOv9:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/WongKinYiu/yolov9
  model-proc: not required
YOLOv10:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/THU-MIG/yolov10
  model-proc: not required
YOLO11:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://docs.ultralytics.com/tasks/detect/
  model-proc: not required
YOLOX:
  source: public
  format: pytorch
  task_type: detection
  readme: https://dlstreamer.github.io/dev_guide/yolo_models.html
  source_readme: https://github.com/Megvii-BaseDetection/YOLOX
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-x.json
ch_PP-OCRv4_rec_infer:
  source: public
  format: paddle
  task_type: ocr
  readme: https://github.com/PaddlePaddle/PaddleOCR
  source_readme: https://github.com/PaddlePaddle/PaddleOCR
  model-proc: not required
CenterFace:
  source: public
  format: onnx
  task_type: detection
  readme: https://github.com/Star-Clouds/CenterFace/tree/master
  source_readme: https://github.com/Star-Clouds/CenterFace/tree/master
  labels-file:
  model-proc: not required
HSEmotion:
  source: public
  format: onnx
  task_type: emotion recognition
  readme: https://github.com/av-savchenko/face-emotion-recognition/tree/main
  source_readme: https://github.com/av-savchenko/face-emotion-recognition/tree/main
  labels-file:
  model-proc: not required

mask_rcnn_inception_resnet_v2_atrous_coco:
  Source framework: TensorFlow\*
  Type: Instance segmentation
  demo_apps:
  - mask_rcnn_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/mask_rcnn_demo/cpp
  description: Mask R-CNN Inception ResNet V2 Atrous  is trained on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset and used for object
    instance segmentation. For details, see a paper <https://arxiv.org/abs/1703.06870>.
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  name: mask_rcnn_inception_resnet_v2_atrous_coco
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mask_rcnn_inception_resnet_v2_atrous_coco
  source: public
  task_type: instance_segmentation
  tf_devices: CPU
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/mask-rcnn.json
mask_rcnn_resnet50_atrous_coco:
  Source framework: TensorFlow\*
  Type: Instance segmentation
  demo_apps:
  - mask_rcnn_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/mask_rcnn_demo/cpp
  description: Mask R-CNN ResNet50 Atrous trained on Common Objects in Context (COCO)
    <https://cocodataset.org/#home> dataset. It is used for object instance segmentation.
    For details, see the paper <https://arxiv.org/abs/1703.06870>.
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  name: mask_rcnn_resnet50_atrous_coco
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mask_rcnn_resnet50_atrous_coco
  source: public
  task_type: instance_segmentation
  tf_devices: CPU
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/mask-rcnn.json

efficientnet-v2-b0:
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: efficientnet-v2-b0
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/efficientnet-v2-b0
  source: public
  task_type: classification
efficientnet-v2-s:
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: efficientnet-v2-s
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/efficientnet-v2-s
  source: public
  task_type: classification

# GETI models
GETI_Detection:
  source: GETI
  task_type: detection
  readme: https://www.intel.com/content/www/us/en/developer/tools/tiber/edge-platform/model-builder.html
  model-proc: not required
GETI_Detection_Oriented:
  source: GETI
  task_type: detection
  readme: https://www.intel.com/content/www/us/en/developer/tools/tiber/edge-platform/model-builder.html
  model-proc: not required
GETI_Instance_Segmentation:
  source: GETI
  task_type: segmentation
  readme: https://www.intel.com/content/www/us/en/developer/tools/tiber/edge-platform/model-builder.html
  model-proc: not required
GETI_Classification_Single_Label:
  source: GETI
  task_type: classification
  readme: https://www.intel.com/content/www/us/en/developer/tools/tiber/edge-platform/model-builder.html
  model-proc: not required
GETI_Classification_Multi_Label:
  source: GETI
  task_type: classification
  readme: https://www.intel.com/content/www/us/en/developer/tools/tiber/edge-platform/model-builder.html
  model-proc: not required

# OpenZoo models
aclnet:
  GFLOPs: '1.42'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - sound_classification_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/sound_classification_demo/python
  description: 'The "AclNet" model is designed to perform sound classification and
    is trained on internal dataset of environmental sounds for 53 different classes,
    listed in file "<omz_dir>/data/dataset_classes/aclnet.txt". For details about
    the model, see this paper <https://arxiv.org/abs/1811.06669>.

    The model input is a segment of PCM audio samples in "N, C, 1, L" format.

    The model output for "AclNet" is the sound classifier output for the 53 different
    environmental sound classes from the internal sound database.'
  license: https://raw.githubusercontent.com/opencv/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/aclnet.json
  name: aclnet
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/aclnet
  source: public
  task_type: sound_classification
aclnet-int8:
  GFLOPs: '2.71'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - sound_classification_demo
  - sound_classification_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/sound_classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/sound_classification_demo/python
  description: 'The "AclNet-int8" model is quantized and fine-tuned with NNCF variant
    of AclNet <https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/aclnet/aclnet.md>
    model, which is designed to perform sound classification. The "AclNet-int8" model
    is trained on an internal dataset of environmental sounds for 53 different classes,
    listed in file "<omz_dir>/data/dataset_classes/aclnet.txt". For details about
    the model, see this paper <https://arxiv.org/abs/1811.06669>.

    The model input is a segment of PCM audio samples in "N, C, 1, L" format.

    The model output for "AclNet-int8" is the sound classifier output for the 53 different
    environmental sound classes from the internal sound database.'
  license: https://raw.githubusercontent.com/opencv/open_model_zoo/master/LICENSE
  name: aclnet-int8
  openvino_devices: CPU
  quantized: INT8
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/aclnet-int8
  source: public
  task_type: sound_classification
action-recognition-0001:
  Source framework: PyTorch\*
  demo_apps:
  - action_recognition_demo
  - action_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  description: General-purpose action recognition model for Kinetics-400 dataset based
    on Video Transformer Network approach.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/kinetics_400.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  module: custom_evaluators.sequential_action_recognition_evaluator.SequentialActionRecognitionEvaluator
  name: action-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/action-recognition-0001
  source: intel
  stages_order:
  - action-recognition-0001-encoder
  - action-recognition-0001-decoder
  task_type: action_recognition
age-gender-recognition-retail-0013:
  '![](./assets/age-gender-recognition-retail-0001.jpg)': Female, 18.97
  '![](./assets/age-gender-recognition-retail-0002.png)': Male, 26.52
  '![](./assets/age-gender-recognition-retail-0003.png)': Male, 33.41
  Avg. age error: 6.99 years
  Gender accuracy: 95.80%
  Min object width: 62 pixels
  Rotation in-plane: "\xB145\u02DA"
  Rotation out-of-plane: "Yaw: \xB145\u02DA / Pitch: \xB145\u02DA"
  Source framework: Caffe\*
  demo_apps:
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Age & gender classification. Used in Audience Analytics.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/age-gender-recognition-retail-0013.json
  name: age-gender-recognition-retail-0013
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/age-gender-recognition-retail-0013
  source: intel
  task_type: object_attributes
anti-spoof-mn3:
  ACER: 3.81%
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  license: https://raw.githubusercontent.com/kirillProkofiev/training_extensions/kp/antispoofing/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/anti-spoof-mn3.json
  name: anti-spoof-mn3
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/anti-spoof-mn3
  source: public
  task_type: classification
asl-recognition-0004:
  Source framework: PyTorch\*
  Top-1 accuracy (MS-ASL-100): '0.847'
  demo_apps:
  - gesture_recognition_demo
  - gesture_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/python
  description: S3D MobileNet V3 based ASL recogniton network.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: asl-recognition-0004
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/asl-recognition-0004
  source: intel
  task_type: action_recognition
background-matting-mobilenetv2:
  Alpha GRAD: '2.48'
  Alpha MAD: '4.32'
  Alpha MSE: '1.0'
  Foreground MSE: '2.7'
  Source framework: PyTorch\*
  Type: Background_matting
  demo_apps:
  - background_subtraction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  description: The "background-matting-mobilenetv2" model is a high-resolution background
    replacement technique based on background matting (with MobileNetV2 backbone),
    where an additional frame of the background is captured and used in recovering
    the alpha matte and the foreground layer. This model is pre-trained in PyTorch*
    framework and converted to ONNX* format. More details provided in the paper <https://arxiv.org/abs/2012.07810>.
    For details see the repository <https://github.com/PeterL1n/BackgroundMattingV2>.
    For details regarding export to ONNX see here <https://github.com/DmitriySidnev/BackgroundMattingV2/blob/master/export_onnx.py>.
  license: https://github.com/DmitriySidnev/BackgroundMattingV2/blob/master/LICENSE
  name: background-matting-mobilenetv2
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/background-matting-mobilenetv2
  source: public
  task_type: background_matting
bert-base-ner:
  Abbreviation: Description
  B-LOC: Beginning of a location right after another location
  B-MIS: Beginning of a miscellaneous entity right after another miscellaneous entity
  B-ORG: Beginning of an organisation right after another organisation
  B-PER: "Beginning of a person\u2019s name right after another person\u2019s name"
  F1: 94.45%
  GOps: '22.3874'
  I-LOC: Location
  I-MIS: Miscellaneous entity
  I-ORG: Organisation
  I-PER: "Person\u2019s name"
  O: Outside of a named entity
  Source framework: PyTorch\*
  license: https://huggingface.co/dslim/bert-base-NER
  name: bert-base-ner
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/bert-base-ner
  source: public
  task_type: named_entity_recognition
bert-large-uncased-whole-word-masking-squad-0001:
  Exact match (EM): 87.20%
  F1: 93.21%
  GOps: '246.93'
  Source framework: PyTorch\*
  description: BERT-large pretrained on lower-cased English text using Whole-Word-Masking
    and fine-tuned on the SQuAD v1.1 training set (93.21% F1 -  87.2% EM on the v1.1
    dev set).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-large-uncased-whole-word-masking-squad-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-large-uncased-whole-word-masking-squad-0001
  source: intel
  task_type: question_answering
bert-large-uncased-whole-word-masking-squad-emb-0001:
  GOps: '246.93'
  Source framework: PyTorch\*
  description: Transformers's bert-large-uncased-whole-word-masking model finetuned
    on SQuAD v1.1 train set to produce question and context embeddings that are close
    to each other in case of the question answer in the context and far from each
    other otherwise.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-large-uncased-whole-word-masking-squad-emb-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-large-uncased-whole-word-masking-squad-emb-0001
  source: intel
  task_type: question_answering
  top5: 90.5%
bert-large-uncased-whole-word-masking-squad-int8-0001:
  Exact match (EM): 86.36%
  F1: 92.60%
  GOps: '246.93'
  Source framework: PyTorch\*
  description: BERT-large pretrained on lower-cased English text using Whole-Word-Masking
    -  fine-tuned on the SQuAD v1.1 training set and symmetrically quantized to INT8
    -  92.60% F1 -  86.36% EM on the v1.1 dev set).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-large-uncased-whole-word-masking-squad-int8-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-large-uncased-whole-word-masking-squad-int8-0001
  source: intel
  task_type: question_answering
bert-small-uncased-whole-word-masking-squad-0001:
  Exact match (EM): 85.04%
  F1: 91.57%
  GOps: '23.9'
  Source framework: PyTorch\*
  description: BERT like small model distilled on the SQuAD v1.1 train set from the
    BERT-large (transformers's bert-large-uncased-whole-word-masking-finetuned-squad)
    pretrained on lower-cased English text using Whole-Word-Masking and fine-tuned
    on the SQuAD v1.1 training set (91.57% F1 -  85.04% EM on the v1.1 dev set).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-small-uncased-whole-word-masking-squad-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-small-uncased-whole-word-masking-squad-0001
  source: intel
  task_type: question_answering
bert-small-uncased-whole-word-masking-squad-0002:
  Exact match (EM): 85.4%
  F1: 91.9%
  GOps: '23.9'
  Source framework: PyTorch\*
  description: BERT like small model distilled on the SQuAD v1.1 train set from the
    BERT-large (transformers's bert-large-uncased-whole-word-masking-finetuned-squad)
    pretrained on lower-cased English text using Whole-Word-Masking and fine-tuned
    on the SQuAD v1.1 training set (91.9% F1 -  85.4% EM on the v1.1 dev set).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-small-uncased-whole-word-masking-squad-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-small-uncased-whole-word-masking-squad-0002
  source: intel
  task_type: question_answering
bert-small-uncased-whole-word-masking-squad-emb-int8-0001:
  GOps: '23.9'
  Source framework: PyTorch\*
  description: Transformers's bert-large-uncased-whole-word-masking model finetuned
    on SQuAD v1.1 train set then distilled to smaller model on SQuAD v1.1 train set
    and then quantized in symmetrical INT8 on SQuAD v1.1 train set to produce question
    and context embeddings that are close to each other in case of the question answer
    in the context and far from each other otherwise.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-small-uncased-whole-word-masking-squad-emb-int8-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-small-uncased-whole-word-masking-squad-emb-int8-0001
  source: intel
  task_type: question_answering
  top5: 87.6%
bert-small-uncased-whole-word-masking-squad-int8-0002:
  Exact match (EM): 84.4%
  F1: 91.4%
  GOps: '23.9'
  Source framework: PyTorch\*
  description: BERT like small symmetrically quantized to INT8 model distilled on
    the SQuAD v1.1 train set from the BERT-large pretrained on lower-cased English
    text using Whole-Word-Masking (transformers's bert-large-uncased-whole-word-masking)
    that is fine-tuned on the SQuAD v1.1 training. (91.4% F1 -  84.4% EM on the v1.1
    dev set).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: bert-small-uncased-whole-word-masking-squad-int8-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002
  source: intel
  task_type: question_answering
brain-tumor-segmentation-0002:
  GFLOPs: '300.801'
  Source framework: PyTorch\*
  Type: Segmentation
  demo_apps:
  - 3d_segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/3d_segmentation_demo/python
  description: This model was created for participation in the Brain Tumor Segmentation
    Challenge <https://www.med.upenn.edu/cbica/brats2019/registration.html> (BraTS)
    2019. It has the UNet architecture trained with residual blocks.
  license: https://raw.githubusercontent.com/lachinov/brats2019/master/LICENSE
  name: brain-tumor-segmentation-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/brain-tumor-segmentation-0002
  source: public
  task_type: semantic_segmentation
cocosnet:
  FID: '33.27'
  GFLOPs: '1080.7032'
  IS: '13.34'
  PSNR: 12.99dB
  SSIM: '0.34'
  Source framework: PyTorch\*
  Type: Image translation
  demo_apps:
  - image_translation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_translation_demo/python
  description: Cross-domain correspondence network is a exemplar-based image translation
    model, consisting of correspondence and translation parts. Model was pre-trained
    on ADE20k dataset. For details see paper <https://arxiv.org/abs/2004.05571> and
    repository <https://github.com/microsoft/CoCosNet>.
  license: https://raw.githubusercontent.com/microsoft/CoCosNet/33f98d092407094a15a08b0555d6f5359490cd3e/LICENSE
  module: custom_evaluators.cocosnet_evaluator.CocosnetEvaluator
  name: cocosnet
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/cocosnet
  source: public
  task_type: image_translation
colorization-siggraph:
  Accuracy top-1: 58.25%
  Accuracy top-5: 81.78%
  GFLOPs: '150.5441'
  PSNR: 27.73dB
  SSIM: '0.92'
  Source framework: PyTorch\*
  Type: Colorization
  demo_apps:
  - colorization_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/colorization_demo/python
  description: 'The "colorization-siggraph" model is one of the colorization <https://arxiv.org/abs/1705.02999>
    group of models designed to real-time user-guided image colorization. Model was
    trained on ImageNet dataset with synthetically generated user interaction. For
    details about this family of models, check out the repository <https://github.com/richzhang/colorization>.

    Model consumes as input L-channel of LAB-image (also user points and binary mask
    as optional inputs). Model give as output predict A- and B-channels of LAB-image.'
  license: https://raw.githubusercontent.com/richzhang/colorization/4f6009ed1495b1300231ebeb41cc4015557ddef7/LICENSE
  name: colorization-siggraph
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/colorization-siggraph
  source: public
  task_type: colorization
colorization-v2:
  Accuracy top-1: 57.75%
  Accuracy top-5: 81.50%
  GFLOPs: '83.6045'
  PSNR: 26.99dB
  SSIM: '0.90'
  Source framework: PyTorch\*
  Type: Colorization
  demo_apps:
  - colorization_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/colorization_demo/python
  description: 'The "colorization-v2" model is one of the colorization <https://arxiv.org/abs/1603.08511>
    group of models designed to perform image colorization. Model was trained on ImageNet
    dataset. For details about this family of models, check out the repository <https://github.com/richzhang/colorization>.

    Model consumes as input L-channel of LAB-image. Model give as output predict A-
    and B-channels of LAB-image.'
  license: https://raw.githubusercontent.com/richzhang/colorization/4f6009ed1495b1300231ebeb41cc4015557ddef7/LICENSE
  name: colorization-v2
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/colorization-v2
  source: public
  task_type: colorization
common-sign-language-0001:
  Source framework: PyTorch\*
  Top-1 accuracy (continuous Jester): 93.58%
  demo_apps:
  - gesture_recognition_demo
  - gesture_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/python
  description: 'A human gesture recognition model for the Jester dataset recognition
    scenario (gesture-level recognition). The model uses an S3D framework with MobileNet
    V3 backbone. Please refer to the Jester <https://20bn.com/datasets/jester> dataset
    specification to see the list of gestures that are recognized by this model.

    The model accepts a stack of frames (8 frames) sampled with a constant frame rate
    (15 FPS) and produces a prediction on the input clip.'
  license: https://github.com/sovrasov/mmaction2/blob/ote/LICENSE
  name: common-sign-language-0001
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/common-sign-language-0001
  source: public
  task_type: action_recognition
common-sign-language-0002:
  Source framework: PyTorch\*
  Top-1 accuracy (continuous CSL): 98.00%
  demo_apps:
  - gesture_recognition_demo
  - gesture_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/python
  description: S3D MobileNet V3 based CSL (Common Sign Language) gesture recogniton
    network.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: common-sign-language-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/common-sign-language-0002
  source: intel
  task_type: action_recognition
convnext-tiny:
  GFLOPs: '8.9419'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'The "convnext-tiny" model is tiny version of ConvNeXt model, constructed
    entirely from standard ConvNet modules. ConvNeXt is accurate, efficient, scalable
    and very simple in design. The model is pre-trained for image classification task
    on the ImageNet dataset.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/rwightman/pytorch-image-models>
    and paper <https://arxiv.org/abs/2201.03545>.'
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  name: convnext-tiny
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/convnext-tiny
  source: public
  task_type: classification
ctdet_coco_dlav0_512:
  Source framework: PyTorch\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: CenterNet object detection model "ctdet_coco_dlav0_512" originally
    trained with PyTorch*. CenterNet models an object as a single point - the center
    point of its bounding box and uses keypoint estimation to find center points and
    regresses to object size. For details see paper <https://arxiv.org/abs/1904.07850>,
    repository <https://github.com/xingyizhou/CenterNet/>.
  license: https://raw.githubusercontent.com/xingyizhou/CenterNet/master/LICENSE
  mAP: 44.2%
  name: ctdet_coco_dlav0_512
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ctdet_coco_dlav0_512
  source: public
  task_type: detection
ctpn:
  Source framework: TensorFlow\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Detecting Text in Natural Image with Connectionist Text Proposal Network.
    For details see paper <https://arxiv.org/abs/1609.03605>.
  hmean: 73.67%
  license: https://raw.githubusercontent.com/eragonruan/text-detection-ctpn/banjin-dev/LICENSE
  name: ctpn
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ctpn
  source: public
  task_type: detection
  tf_devices: CPU
deeplabv3:
  GFLOPs: '11.469'
  Source framework: TensorFlow\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: DeepLab is a state-of-art deep learning model for semantic image segmentation.
    For details see paper <https://arxiv.org/abs/1706.05587>.
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  mean_iou: 68.41%
  name: deeplabv3
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/deeplabv3
  source: public
  task_type: semantic_segmentation
  tf_devices: CPU
densenet-121-tf:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: This is a TensorFlow* version of "densenet-121" model, one of the DenseNet
    group of models designed to perform image classification. For details, see TensorFlow*
    API docs <https://www.tensorflow.org/api_docs/python/tf/keras/applications/DenseNet121>,
    repository <https://github.com/tensorflow/tensorflow> and paper <https://arxiv.org/abs/1608.06993>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/tensorflow/tensorflow/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: densenet-121-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/densenet-121-tf
  source: public
  task_type: classification
  tf_devices: CPU
detr-resnet50:
  GFLOPs: '174.4708'
  Source framework: PyTorch\*
  Type: Object detection
  coco_orig_precision: 39.27%
  coco_precision: 42.36%
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'The "detr-resnet50" model is one from DEtection TRansformer (DETR)
    models family, which consider object detection as a direct set prediction problem.
    The model has ResNet50 backbone and pretrained on Common Objects in Context <COCO>
    <https://cocodataset.org/#home> dataset for solving object detection task. DETR
    predicts all objects at once, and is trained end-to-end with a set loss function
    which performs bipartite matching between predicted and ground-truth objects.
    DETR simplifies the detection pipeline by dropping multiple hand-designed components
    that encode prior knowledge, like spatial anchors or non-maximal suppression.

    More details provided in the paper <https://arxiv.org/abs/2005.12872> and repository
    <https://github.com/facebookresearch/detr>.'
  license: https://raw.githubusercontent.com/facebookresearch/detr/master/LICENSE
  name: detr-resnet50
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/detr-resnet50
  source: public
  task_type: detection
dla-34:
  GFLOPs: '6.1368'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "dla-34" model is one of the DLA <https://arxiv.org/pdf/1707.06484>
    models designed to perform image classification. This model was pre-trained in
    PyTorch*. All DLA (Deep Layer Aggregation) classification models have been pre-trained
    on the ImageNet dataset. For details about this family of models, check out the
    Code for the CVPR Paper "Deep Layer Aggregation" <https://github.com/ucbdrive/dla>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/ucbdrive/dla/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: dla-34
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/dla-34
  source: public
  task_type: classification
driver-action-recognition-adas-0002:
  Source framework: PyTorch\*
  demo_apps:
  - action_recognition_demo
  - action_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  description: Video Transformer Network for driver action recognition.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/driver_actions.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  module: custom_evaluators.sequential_action_recognition_evaluator.SequentialActionRecognitionEvaluator
  name: driver-action-recognition-adas-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/driver-action-recognition-adas-0002
  source: intel
  stages_order:
  - driver-action-recognition-adas-0002-encoder
  - driver-action-recognition-adas-0002-decoder
  task_type: action_recognition
drn-d-38:
  GFLOPs: '1768.3276'
  Source framework: PyTorch\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: 'The "drn-d-38" model is a one of the Dilated Residual Networks (DRN)
    models for semantic segmentation task. DRN models dilate ResNet models, DRN-C
    version additionally removes residual connections from some of the added blocks
    and DRN-D version is a simplified version of DRN-C.

    This model pre-trained on Cityscapes <https://www.cityscapes-dataset.com> dataset
    for 19 object classes, listed in "<omz_dir>/data/dataset_classes/cityscapes_19cl.txt"
    file. See Cityscapes classes definition <https://www.cityscapes-dataset.com/dataset-overview>
    for more details.

    More details provided in the paper <https://arxiv.org/abs/1705.09914> and repository
    <https://github.com/fyu/drn>.'
  license: https://raw.githubusercontent.com/fyu/drn/master/LICENSE
  mean_iou: 71.31%
  name: drn-d-38
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/drn-d-38
  source: public
  task_type: semantic_segmentation
efficientdet-d0-tf:
  GFLOPs: '2.54'
  Source framework: TensorFlow\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: The "efficientdet-d0-tf" model is one of the EfficientDet <https://arxiv.org/abs/1911.09070>
    models  designed to perform object detection. This model was pre-trained in TensorFlow*.
    All the EfficientDet models have been pre-trained on the Common Objects in Context
    (COCO) <https://cocodataset.org/#home> image database. For details about this
    family of models, check out the Google AutoML repository <https://github.com/google/automl/tree/master/efficientdet>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl.txt
  license: https://raw.githubusercontent.com/google/automl/master/LICENSE
  name: efficientdet-d0-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/efficientdet-d0-tf
  source: public
  task_type: detection
  tf_devices: CPU
efficientdet-d1-tf:
  GFLOPs: '6.1'
  Source framework: TensorFlow\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: The "efficientdet-d1-tf" model is one of the EfficientDet <https://arxiv.org/abs/1911.09070>
    models  designed to perform object detection. This model was pre-trained in TensorFlow*.
    All the EfficientDet models have been pre-trained on the Common Objects in Context
    (COCO) <https://cocodataset.org/#home> image database. For details about this
    family of models, check out the Google AutoML repository <https://github.com/google/automl/tree/master/efficientdet>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl.txt
  license: https://raw.githubusercontent.com/google/automl/master/LICENSE
  name: efficientdet-d1-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/efficientdet-d1-tf
  source: public
  task_type: detection
  tf_devices: CPU
efficientnet-b0:
  GFLOPs: '0.819'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "efficientnet-b0" model is one of the EfficientNet <https://arxiv.org/abs/1905.11946>
    models designed to perform image classification. This model was pre-trained in
    TensorFlow*. All the EfficientNet models have been pre-trained on the ImageNet
    image database. For details about this family of models, check out the TensorFlow
    Cloud TPU repository <https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/tensorflow/tpu/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: efficientnet-b0
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/efficientnet-b0
  source: public
  task_type: classification
  tf_devices: CPU
efficientnet-b0-pytorch:
  GFLOPs: '0.819'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'The "efficientnet-b0-pytorch" model is one of the EfficientNet <https://arxiv.org/abs/1905.11946>
    models designed to perform image classification. This model was pre-trained in
    PyTorch*. All the EfficientNet models have been pre-trained on the ImageNet image
    database. For details about this family of models, check out the EfficientNets
    for PyTorch repository <https://github.com/rwightman/gen-efficientnet-pytorch>.

    The model input is a blob that consists of a single image with the "3, 224, 224"
    shape in the "RGB" order. Before passing the image blob to the network, do the
    following: 1. Subtract the RGB mean values as follows: [123.675, 116.28, 103.53]
    2. Divide the RGB mean values by  [58.395, 57.12, 57.375]

    The model output for "efficientnet-b0-pytorch" is the typical object classifier
    output for 1000 different classifications matching those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/rwightman/gen-efficientnet-pytorch/a36e2b2cd1bd122a508a6fffeaa7606890f8c882/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: efficientnet-b0-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/efficientnet-b0-pytorch
  source: public
  task_type: classification

emotions-recognition-retail-0003:
  '![](./assets/emotions-recognition-retail-0003.jpg)': Happiness
  Input face orientation: Frontal
  Min object width: 64 pixels
  Rotation in-plane: "\xB115\u02DA"
  Rotation out-of-plane: "Yaw: \xB115\u02DA / Pitch: \xB115\u02DA"
  Source framework: Caffe\*
  demo_apps:
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Recognizes 5 emotions for a face. Targeted for Retail Audience Analytics.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/emotions-recognition-retail-0003.json
  name: emotions-recognition-retail-0003
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/emotions-recognition-retail-0003
  source: intel
  task_type: object_attributes
erfnet:
  GFLOPs: '11.13'
  Source framework: PyTorch\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: This is a ONNX* version of `ERFNet` model designed to perform real-time
    lane detection. This model is pre-trained in PyTorch* framework and retrained
    by CULane. For details see repository <https://github.com/Zhangxianwen2021/ERFNet>,
    paper of ERFNet<https://doi.org/10.1109/TITS.2017.2750080>and repository <https://github.com/cardwing/Codes-for-Lane-Detection/tree/master/ERFNet-CULane-PyTorch>
  license: https://raw.githubusercontent.com/Zhangxianwen2021/ERFNet/main/License
  mean_iou: 76.47%
  name: erfnet
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/erfnet
  source: public
  task_type: semantic_segmentation
f3net:
  F-measure: 84.21%
  GFLOPs: '31.2883'
  Source framework: PyTorch\*
  Type: Salient object detection
  demo_apps:
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: 'F3Net: Fusion, Feedback and Focus for Salient Object Detection. For
    details see the repository <https://github.com/weijun88/F3Net>, paper <https://arxiv.org/abs/1911.11445>'
  license: https://github.com/weijun88/F3Net/blob/master/LICENSE
  name: f3net
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/f3net
  source: public
  task_type: salient_object_detection
face-detection-0200:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 86.74%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Face Detection based on MobileNetV2 (SSD).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-0200.json
  name: face-detection-0200
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-0200
  source: intel
  task_type: detection
face-detection-0202:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 91.94%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Face Detection based on MobileNetV2 (SSD).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-0202.json
  name: face-detection-0202
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-0202
  source: intel
  task_type: detection
face-detection-0204:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 92.89%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Face Detection based on MobileNetV2 (SSD).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-0204.json
  name: face-detection-0204
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-0204
  source: intel
  task_type: detection
face-detection-0205:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 93.57%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Face Detection based on MobileNetV2 (FCOS).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-0205.json
  name: face-detection-0205
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-0205
  source: intel
  task_type: detection
face-detection-0206:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 94.27%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Face Detection based on ResNet152 (ATSS).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-0206.json
  name: face-detection-0206
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-0206
  source: intel
  task_type: detection
face-detection-adas-0001:
  AP (head height >100px): 94.1%
  AP (head height >10px): 37.4%
  AP (head height >32px): 84.8%
  AP (head height >64px): 93.1%
  Min head size: 90x90 pixels on 1080p
  Source framework: Caffe\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - multi_channel_face_detection_demo
  - gaze_estimation_demo
  - gaze_estimation_demo
  - face_recognition_demo
  - smart_classroom_demo
  - smart_classroom_demo
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_face_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Face Detection (MobileNet with reduced channels + SSD with weights
    sharing)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-adas-0001.json
  name: face-detection-adas-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-adas-0001
  source: intel
  task_type: detection
face-detection-retail-0004:
  AP ([WIDER](http://shuoyang1213.me/WIDERFACE/)): 83.00%
  Source framework: Caffe\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - multi_channel_face_detection_demo
  - gaze_estimation_demo
  - gaze_estimation_demo
  - face_recognition_demo
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_face_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Face Detection (SqNet1.0modif+single scale) without BatchNormalization
    trained with negatives.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-retail-0004.json
  name: face-detection-retail-0004
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-retail-0004
  source: intel
  task_type: detection
face-detection-retail-0005:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 84.52%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - multi_channel_face_detection_demo
  - gaze_estimation_demo
  - gaze_estimation_demo
  - face_recognition_demo
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_face_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Face Detection based on MobileNetV2.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/face-detection-retail-0005.json
  name: face-detection-retail-0005
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-detection-retail-0005
  source: intel
  task_type: detection
face-recognition-resnet100-arcface-onnx:
  GFLOPs: '24.2115'
  LFW accuracy: 99.68%
  Source framework: MXNet\*
  Type: Face recognition
  demo_apps:
  - face_recognition_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: The "face-recognition-resnet100-arcface-onnx" model is a deep face
    recognition model with ResNet100 backbone and ArcFace loss. ArcFace is a novel
    supervisor signal called additive angular margin which used as an additive term
    in the softmax loss to enhance the discriminative power of softmax loss. This
    model is pre-trained in MXNet* framework and converted to ONNX* format. More details
    provided in the paper <https://arxiv.org/abs/1801.07698> and repository <https://github.com/onnx/models/tree/master/vision/body_analysis/arcface>.
  license: https://raw.githubusercontent.com/onnx/models/master/LICENSE
  name: face-recognition-resnet100-arcface-onnx
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/face-recognition-resnet100-arcface-onnx
  source: public
  task_type: face_recognition
face-reidentification-retail-0095:
  Face location requirements: Tight aligned crop
  LFW accuracy: '0.9947'
  Source framework: PyTorch\*
  demo_apps:
  - face_recognition_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Single embedding-based face verification model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: face-reidentification-retail-0095
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/face-reidentification-retail-0095
  source: intel
  task_type: face_recognition
faceboxes-pytorch:
  GFLOPs: '1.8975'
  Source framework: PyTorch\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'FaceBoxes: A CPU Real-time Face Detector with High Accuracy. For details
    see the repository <https://github.com/zisianw/FaceBoxes.PyTorch>, paper <https://arxiv.org/abs/1708.05234>'
  license: https://github.com/zisianw/FaceBoxes.PyTorch/blob/master/LICENSE
  mAP: 83.565%
  name: faceboxes-pytorch
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/faceboxes-pytorch
  source: public
  task_type: detection
facenet-20180408-102900:
  LFW accuracy: 99.14%
  Source framework: TensorFlow\*
  Type: Face recognition
  demo_apps:
  - face_recognition_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: 'FaceNet: A Unified Embedding for Face Recognition and Clustering.
    For details see the repository <https://github.com/davidsandberg/facenet/>, paper
    <https://arxiv.org/abs/1503.03832>'
  license: https://raw.githubusercontent.com/davidsandberg/facenet/master/LICENSE.md
  name: facenet-20180408-102900
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/facenet-20180408-102900
  source: public
  task_type: face_recognition
  tf_devices: CPU
facial-landmarks-35-adas-0002:
  Internal dataset: '0.106'
  Source framework: Caffe\*
  demo_apps:
  - gaze_estimation_demo
  - gaze_estimation_demo
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Custom-architecture convolutional neural network for 35 facial landmarks
    estimation.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/facial-landmarks-35-adas-0002.json
  name: facial-landmarks-35-adas-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/facial-landmarks-35-adas-0002
  source: intel
  task_type: object_attributes
facial-landmarks-98-detection-0001:
  NME: '0.1323'
  Source framework: PyTorch\*
  demo_apps:
  - gaze_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  description: Landmark's detection. 98 points. Trained on the internal dataset
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/facial-landmarks-98-detection-0001.json
  name: facial-landmarks-98-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/facial-landmarks-98-detection-0001
  source: intel
  task_type: object_attributes
fast-neural-style-mosaic-onnx:
  GFLOPs: '15.518'
  PSNR: 12.03dB
  Source framework: PyTorch\*
  Type: Style Transfer
  demo_apps:
  - image_processing_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_processing_demo/cpp
  description: The "fast-neural-style-mosaic-onnx" model is one of the style transfer
    models designed to mix the content of an image with the style of another image.
    The model uses the method described in Perceptual Losses for Real-Time Style Transfer
    and Super-Resolution <https://arxiv.org/abs/1603.08155> along with Instance Normalization
    <https://arxiv.org/abs/1607.08022>. Original ONNX models are provided in the repository
    <https://github.com/onnx/models>.
  license: https://raw.githubusercontent.com/onnx/models/master/LICENSE
  name: fast-neural-style-mosaic-onnx
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/fast-neural-style-mosaic-onnx
  source: public
  task_type: style_transfer
faster-rcnn-resnet101-coco-sparse-60-0001:
  Mean Average Precision (mAP): 38.74%\**
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Faster RCNN with ResNet101 backbone trained on COCO dataset with 0.6
    sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: faster-rcnn-resnet101-coco-sparse-60-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/faster-rcnn-resnet101-coco-sparse-60-0001
  source: intel
  task_type: detection
faster_rcnn_inception_resnet_v2_atrous_coco:
  Source framework: TensorFlow\*
  Type: Object detection
  coco_precision: 40.69%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Faster R-CNN with Inception ResNet v2 Atrous version. Used for object
    detection. For details see the paper <https://arxiv.org/abs/1506.01497>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-image-info.json
  name: faster_rcnn_inception_resnet_v2_atrous_coco
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/faster_rcnn_inception_resnet_v2_atrous_coco
  source: public
  task_type: detection
  tf_devices: CPU
faster_rcnn_resnet50_coco:
  Source framework: TensorFlow\*
  Type: Object detection
  coco_precision: 31.09%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Faster R-CNN ResNet-50 model. Used for object detection. For details,
    see the paper <https://arxiv.org/abs/1506.01497>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-image-info.json
  name: faster_rcnn_resnet50_coco
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/faster_rcnn_resnet50_coco
  source: public
  task_type: detection
  tf_devices: CPU
fastseg-large:
  GOps: '140.9611'
  Source framework: PyTorch\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: fastseg-large is an accurate real-time semantic segmentation model,
    pre-trained on Cityscapes <https://www.cityscapes-dataset.com> dataset for 19
    object classes, listed in "<omz_dir>/data/dataset_classes/cityscapes_19cl_bkgr.txt"
    file. See Cityscapes classes definition <https://www.cityscapes-dataset.com/dataset-overview>
    for more details. The model was built on MobileNetV3 large backbone and modified
    segmentation head based on LR-ASPP. This model can be used for efficient segmentation
    on a variety of real-world street images. For model implementation details see
    original repository <https://github.com/ekzhang/fastseg>.
  license: https://raw.githubusercontent.com/ekzhang/fastseg/master/LICENSE.txt
  mean_iou: 72.67%
  name: fastseg-large
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/fastseg-large
  source: public
  task_type: semantic_segmentation
fastseg-small:
  GOps: '69.2204'
  Source framework: PyTorch\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: fastseg-small is an accurate real-time semantic segmentation model,
    pre-trained on Cityscapes <https://www.cityscapes-dataset.com> dataset for 19
    object classes, listed in "<omz_dir>/data/dataset_classes/cityscapes_19cl_bkgr.txt"
    file. See Cityscapes classes definition <https://www.cityscapes-dataset.com/dataset-overview>
    for more details. The model was built on MobileNetV3 small backbone and modified
    segmentation head based on LR-ASPP. This model can be used for efficient segmentation
    on a variety of real-world street images. For model implementation details see
    original repository <https://github.com/ekzhang/fastseg>.
  license: https://raw.githubusercontent.com/ekzhang/fastseg/master/LICENSE.txt
  mean_iou: 67.15%
  name: fastseg-small
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/fastseg-small
  source: public
  task_type: semantic_segmentation
fbcnn:
  GFLOPs: '1420.78235'
  PSNR: 34.34Db
  SSIM: '0.99'
  Source framework: PyTorch\*
  Type: Image Processing
  demo_apps:
  - image_processing_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_processing_demo/cpp
  description: The "fbcnn" model is a flexible blind convolutional neural network
    to remove JPEG artifacts. Model based on "Towards Flexible Blind JPEG Artifacts
    Removal" <https://arxiv.org/abs/2109.14573> paper. It was implemented in PyTorch*
    framework. Model works with color jpeg images. For details about this model and
    other jpeg artifacts removal models (for grayscale images and double jpeg restoration),
    check out the "Towards Flexible Blind JPEG Artifacts Removal (FBCNN, ICCV 2021)"
    <https://github.com/jiaxi-jiang/FBCNN>.
  license: https://raw.githubusercontent.com/jiaxi-jiang/FBCNN/main/LICENSE
  name: fbcnn
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/fbcnn
  source: public
  task_type: image_processing
fcrn-dp-nyu-depth-v2-tf:
  GFLOPs: '63.5421'
  Source framework: TensorFlow\*
  Type: Monodepth
  '[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)': '0.573'
  demo_apps:
  - monodepth_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/monodepth_demo/python
  description: This is a model for monocular depth estimation trained on the NYU Depth
    V2 dataset, as described in the paper Deeper Depth Prediction with Fully Convolutional
    Residual Networks <https://arxiv.org/abs/1606.00373>, where it is referred to
    as ResNet-UpProj. The model input is a single color image. The model output is
    an inverse depth map that is defined up to an unknown scale factor. More details
    can be found in the following repository <https://github.com/iro-cp/FCRN-DepthPrediction>.
  license: https://raw.githubusercontent.com/iro-cp/FCRN-DepthPrediction/master/LICENSE
  log10: '0.055'
  name: fcrn-dp-nyu-depth-v2-tf
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/fcrn-dp-nyu-depth-v2-tf
  rel: '0.127'
  source: public
  task_type: monocular_depth_estimation
  tf_devices: CPU
formula-recognition-medium-scan-0001:
  Source framework: PyTorch\*
  demo_apps:
  - formula_recognition_demo
  - formula_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/formula_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/formula_recognition_demo/python
  description: Latex formula recognition model.
  im2latex_medium_photographed dataset, im2latex-match-images metric: 81.5%
  im2latex_medium_rendered dataset, im2latex-match-images metric: 95.7%
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  module: custom_evaluators.custom_text_recognition_evaluator.TextRecognitionWithAttentionEvaluator
  name: formula-recognition-medium-scan-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/formula-recognition-medium-scan-0001
  source: intel
  stages_order:
  - formula-recognition-medium-scan-0001-im2latex-encoder
  - formula-recognition-medium-scan-0001-im2latex-decoder
  task_type: token_recognition
formula-recognition-polynomials-handwritten-0001:
  Source framework: PyTorch\*
  demo_apps:
  - formula_recognition_demo
  - formula_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/formula_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/formula_recognition_demo/python
  description: Latex formula recognition model.
  im2latex_polynomials_handwritten dataset, im2latex-match-images metric: 70.5%
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  module: custom_evaluators.custom_text_recognition_evaluator.TextRecognitionWithAttentionEvaluator
  name: formula-recognition-polynomials-handwritten-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/formula-recognition-polynomials-handwritten-0001
  source: intel
  stages_order:
  - formula-recognition-polynomials-handwritten-0001-encoder
  - formula-recognition-polynomials-handwritten-0001-decoder
  task_type: token_recognition
forward-tacotron:
  GOPs: '4.91'
  Source framework: PyTorch\*
  demo_apps:
  - text_to_speech_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_to_speech_demo/python
  description: ForwardTacotron model for text to speech task.
  license: https://github.com/as-ideas/ForwardTacotron/blob/master/LICENSE
  name: forward-tacotron
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/forward-tacotron
  source: public
  stages_order:
  - forward-tacotron-duration-prediction
  - forward-tacotron-regression
  task_type: text_to_speech
gaze-estimation-adas-0002:
  Internal dataset: '6.95'
  Source framework: PyTorch\*
  demo_apps:
  - gaze_estimation_demo
  - gaze_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  description: Gaze estimation for ADAS
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: gaze-estimation-adas-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/gaze-estimation-adas-0002
  source: intel
  task_type: object_attributes
gmcnn-places2-tf:
  PSNR: 33.41dB
  Source framework: TensorFlow\*
  Type: Image Inpainting
  demo_apps:
  - image_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_inpainting_demo/python
  description: The "gmcnn-places2-tf" is the TensorFlow* implementation of GMCNN Image
    Inpainting model, aimed to estimate suitable pixel information to fill holes in
    images. "gmcnn-places2-tf" is trained on Places2 dataset with free-form masks.
    Originally redistributed as checkpoint files, it was converted to a frozen graph.
    For details see repository <https://github.com/shepnerd/inpainting_gmcnn>.
  license: https://raw.githubusercontent.com/shepnerd/inpainting_gmcnn/master/LICENSE
  name: gmcnn-places2-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/gmcnn-places2-tf
  source: public
  task_type: image_inpainting
  tf_devices: CPU
googlenet-v1-tf:
  GFLOPs: '3.016'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "googlenet-v1-tf" model is one of the Inception family, designed
    to perform image classification. Like the other Inception models, the "googlenet-v1-tf"
    model has been pre-trained on the ImageNet image database. For details about this
    family of models, check out the paper <https://arxiv.org/abs/1602.07261>, repository
    <https://github.com/tensorflow/models/tree/master/research/slim>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://github.com/tensorflow/models/blob/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: googlenet-v1-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/googlenet-v1-tf
  source: public
  task_type: classification
  tf_devices: CPU
googlenet-v2-tf:
  GFLOPs: '4.058'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "googlenet-v2-tf" model is one of the Inception family, designed
    to perform image classification. Like the other Inception models, the "googlenet-v2-tf"
    model has been pre-trained on the ImageNet image database. For details about this
    family of models, check out the paper <https://arxiv.org/abs/1602.07261>, repository
    <https://github.com/tensorflow/models/tree/master/research/slim>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://github.com/tensorflow/models/blob/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: googlenet-v2-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/googlenet-v2-tf
  source: public
  task_type: classification
  tf_devices: CPU
googlenet-v3:
  GFLOPs: '11.469'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "googlenet-v3" model is the first of the Inception family of models
    designed to perform image classification. For details about this family of models,
    check out the paper <https://arxiv.org/abs/1602.07261>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: googlenet-v3
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/googlenet-v3
  source: public
  task_type: classification
  tf_devices: CPU
googlenet-v3-pytorch:
  GFLOPs: '11.469'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: googlenet-v3-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/googlenet-v3-pytorch
  source: public
  task_type: classification
googlenet-v4-tf:
  GFLOPs: '24.584'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "googlenet-v4-tf" model is the most recent of the Inception family
    of models designed to perform image classification. Like the other Inception models,
    the "googlenet-v4-tf" model has been pre-trained on the ImageNet image database.
    For details about this family of models, check out the paper <https://arxiv.org/abs/1602.07261>,
    repository <https://github.com/tensorflow/models/tree/master/research/slim>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://github.com/tensorflow/models/blob/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: googlenet-v4-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/googlenet-v4-tf
  source: public
  task_type: classification
  tf_devices: CPU
gpt-2:
  Perplexity: 29.00%
  Source framework: PyTorch\*
  Type: Text Prediction
  demo_apps:
  - gpt2_text_prediction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gpt2_text_prediction_demo/python
  description: 'The "gpt-2" model is a one of Generative Pre-trained Transformer (GPT)
    model family, pre-trained on a very large corpus of English data in a self-supervised
    fashion. The GPT architecture implements a deep neural network, specifically a
    transformer model, which uses attention in place of previous recurrence- and convolution-based
    architectures. Attention mechanisms allow the model to selectively focus on segments
    of input text it predicts to be the most relevant. GPT-2 is trained with a simple
    objective: predict the next word, given all of the previous words within some
    text.

    More details provided in the paper <https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>,
    repository <https://github.com/huggingface/transformers> and model card <https://huggingface.co/gpt2>.'
  license: https://huggingface.co/gpt2
  name: gpt-2
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/gpt-2
  source: public
  task_type: text_prediction
handwritten-english-recognition-0001:
  Accuracy on GNHK test subset (excluding images wider than 2000px after resized to height 96px with aspect ratio): 82.0%
  Source framework: PyTorch\*
  demo_apps:
  - handwritten_text_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/handwritten_text_recognition_demo/python
  description: Recognizes handwritten English text. Architecture is CNN for feature
    extraction followed by RNN for sequence modeling.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: handwritten-english-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/handwritten-english-recognition-0001
  source: intel
  task_type: optical_character_recognition
handwritten-japanese-recognition-0001:
  Accuracy on Kondate test set and test set generated from Nakayosi: 98.16%
  Source framework: PyTorch\*
  demo_apps:
  - handwritten_text_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/handwritten_text_recognition_demo/python
  description: 'Recognizes handwritten Japanese text. Architecture is CNN-like: VGG16-like
    backbone.'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: handwritten-japanese-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/handwritten-japanese-recognition-0001
  source: intel
  task_type: optical_character_recognition
handwritten-score-recognition-0003:
  Accuracy (internal test set): 98.83%
  Source framework: TensorFlow\*
  Text location requirements: Tight aligned crop
  demo_apps:
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: 'Recognizes school marks (e.g ''4'' or ''3.5''). Alphabet is ''0123456789._''.
    Architecture: VGG-like + BiLSTM as an encoder -  BiLSTM as a decoder.'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: handwritten-score-recognition-0003
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/handwritten-score-recognition-0003
  source: intel
  task_type: optical_character_recognition
handwritten-simplified-chinese-recognition-0001:
  Accuracy on SCUT-EPT test subset (excluding images wider than 2000px after resized to height 96px with aspect ratio): 75.31%
  Source framework: PyTorch\*
  demo_apps:
  - handwritten_text_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/handwritten_text_recognition_demo/python
  description: 'Recognizes handwritten simplified Chinese text. Architecture is CNN-like:
    VGG16-like backbone.'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: handwritten-simplified-chinese-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/handwritten-simplified-chinese-recognition-0001
  source: intel
  task_type: optical_character_recognition
hbonet-0.25:
  GFLOPs: '0.037'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "hbonet-0.25" model is one of the classification models from repository
    <https://github.com/d-li14/HBONet> with "width_mult=0.25"
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://github.com/d-li14/HBONet/blob/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: hbonet-0.25
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/hbonet-0.25
  source: public
  task_type: classification
hbonet-1.0:
  GFLOPs: '0.305'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "hbonet-1.0" model is one of the classification models from repository
    <https://github.com/d-li14/HBONet> with "width_mult=1.0"
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://github.com/d-li14/HBONet/blob/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: hbonet-1.0
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/hbonet-1.0
  source: public
  task_type: classification
head-pose-estimation-adas-0001:
  Angle: "[Mean](https://en.wikipedia.org/wiki/Mean_absolute_error) \xB1 [standard\
    \ deviation](https://en.wikipedia.org/wiki/Standard_deviation) of absolute error"
  Source framework: Caffe\*
  Supported ranges: YAW [-90,90], PITCH [-70,70], ROLL [-70,70]
  demo_apps:
  - gaze_estimation_demo
  - gaze_estimation_demo
  - interactive_face_detection_demo
  - interactive_face_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/interactive_face_detection_demo/cpp
  description: Vanilla CNN trained from scratch yaw + pitch + roll + landmarks
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: head-pose-estimation-adas-0001
  openvino_devices: CPU, GPU
  pitch: "5.5 \xB1 5.3"
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/head-pose-estimation-adas-0001
  roll: "4.6 \xB1 5.6"
  source: intel
  task_type: head_pose_estimation
  yaw: "5.4 \xB1 4.4"
higher-hrnet-w32-human-pose-estimation:
  Average Precision (AP): 64.64%
  GFLOPs: '92.8364'
  Source framework: PyTorch\*
  Type: Human pose estimation
  demo_apps:
  - human_pose_estimation_demo
  - human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/python
  description: 'The "HigherHRNet-W32" model is one of the HigherHRNet <https://arxiv.org/pdf/1908.10357>.
    "HigherHRNet" is a novel bottom-up human pose estimation method for learning scale-aware
    representations using high-resolution feature pyramids. The network uses HRNet
    as backbone, followed by one or more deconvolution modules to generate multi-resolution
    and high-resolution heatmaps. For every person in an image, the network detects
    a human pose: a body skeleton consisting of keypoints and connections between
    them. The pose may contain up to 17 keypoints: ears, eyes, nose, shoulders, elbows,
    wrists, hips, knees, and ankles. This is PyTorch* implementation pre-trained on
    COCO dataset. For details about implementation of model, check out the HigherHRNet:
    Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation <https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation>
    repository.'
  license: https://raw.githubusercontent.com/CSAILVision/semantic-segmentation-pytorch/9aff40de31ee4b21f18514d31e5d6e4ba056924d/LICENSE
  name: higher-hrnet-w32-human-pose-estimation
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/higher-hrnet-w32-human-pose-estimation
  source: public
  task_type: human_pose_estimation
horizontal-text-detection-0001:
  F-measure (harmonic mean of precision and recall on ICDAR2013): 88.45%
  Source framework: PyTorch\*
  demo_apps:
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: Horizontal text detector based on FCOS with light MobileNetV2 backbone
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/horizontal-text-detection-0001.json
  name: horizontal-text-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/horizontal-text-detection-0001
  source: intel
  task_type: detection
hrnet-v2-c1-segmentation:
  GFLOPs: '81.9930'
  Pixel accuracy: 77.69%
  Source framework: PyTorch\*
  Type: Segmentation
  demo_apps:
  - image_translation_demo
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_translation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: This model is a pair of encoder and decoder. The encoder is HRNetV2-W48
    and the decoder is C1 (one convolution module and interpolation). HRNetV2-W48
    is semantic-segmentation model based on architecture described in paper High-Resolution
    Representations for Labeling Pixels and Regions <https://arxiv.org/abs/1904.04514>.
    This is PyTorch* implementation based on retaining high resolution representations
    throughout the model and pre-trained on ADE20k dataset. For details about implementation
    of model, check out the Semantic Segmentation on MIT ADE20K dataset in PyTorch
    <https://github.com/CSAILVision/semantic-segmentation-pytorch> repository.
  license: https://raw.githubusercontent.com/CSAILVision/semantic-segmentation-pytorch/9aff40de31ee4b21f18514d31e5d6e4ba056924d/LICENSE
  mean IoU: 33.02%
  name: hrnet-v2-c1-segmentation
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/hrnet-v2-c1-segmentation
  source: public
  task_type: semantic_segmentation
human-pose-estimation-0001:
  Average Precision (AP): 42.8%
  Source framework: Caffe\*
  demo_apps:
  - multi_channel_human_pose_estimation_demo
  - human_pose_estimation_demo
  - human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/python
  description: 2D human pose estimation with tuned mobilenet backbone (based on OpenPose).
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/human-pose-estimation-0001.json
  name: human-pose-estimation-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/human-pose-estimation-0001
  source: intel
  task_type: human_pose_estimation
human-pose-estimation-0005:
  Average Precision (AP): 45.6%
  Source framework: PyTorch\*
  demo_apps:
  - human_pose_estimation_demo
  - human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/python
  description: 2D human pose estimation based on EfficientHRNet.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: human-pose-estimation-0005
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/human-pose-estimation-0005
  source: intel
  task_type: human_pose_estimation
human-pose-estimation-0006:
  Average Precision (AP): 51.1%
  Source framework: PyTorch\*
  demo_apps:
  - human_pose_estimation_demo
  - human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/python
  description: 2D human pose estimation based on EfficientHRNet.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: human-pose-estimation-0006
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/human-pose-estimation-0006
  source: intel
  task_type: human_pose_estimation
human-pose-estimation-0007:
  Average Precision (AP): 54.3%
  Source framework: PyTorch\*
  demo_apps:
  - human_pose_estimation_demo
  - human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_demo/python
  description: 2D human pose estimation based on EfficientHRNet.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: human-pose-estimation-0007
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/human-pose-estimation-0007
  source: intel
  task_type: human_pose_estimation
human-pose-estimation-3d-0001:
  MPJPE (mm): '100.45'
  Source framework: PyTorch\*
  demo_apps:
  - human_pose_estimation_3d_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/human_pose_estimation_3d_demo/python
  description: Multi-person 3D human pose estimation model based on the Lightweight
    OpenPose <https://arxiv.org/abs/1811.12004> and Single-Shot Multi-Person 3D Pose
    Estimation From Monocular RGB <https://arxiv.org/abs/1712.03453> papers.
  license: https://raw.githubusercontent.com/opencv/openvino_training_extensions/develop/LICENSE
  name: human-pose-estimation-3d-0001
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/human-pose-estimation-3d-0001
  source: public
  task_type: human_pose_estimation
hybrid-cs-model-mri:
  PSNR (mean): 34.272884 dB
  PSNR (std): 4.607115 dB
  Source framework: TensorFlow\*
  Type: MRI Image Inpainting in k-Space
  demo_apps:
  - mri_reconstruction_demo
  - mri_reconstruction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/mri_reconstruction_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/mri_reconstruction_demo/python
  description: 'The "hybrid-cs-model-mri" model is a hybrid frequency-domain/image-domain
    deep network for Magnetic Resonance Image (MRI) reconstruction. The model is composed
    of a k-space network that essentially tries to fill missing k-space samples, an
    Inverse Discrete Fourier Transformation (IDFT) operation, and an image-domain
    network that acts as an anti-aliasing filter.

    More details provided in the paper <https://arxiv.org/abs/1810.12473> and repository
    <https://github.com/rmsouza01/Hybrid-CS-Model-MRI>.'
  license: https://raw.githubusercontent.com/rmsouza01/Hybrid-CS-Model-MRI/2ede2f96161ce70dcdc922371fe6b6b254aafcc8/LICENSE
  name: hybrid-cs-model-mri
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/hybrid-cs-model-mri
  source: public
  task_type: image_inpainting
  tf_devices: CPU
i3d-rgb-tf:
  GFLOPs: '278.981'
  Source framework: TensorFlow\*
  Type: Action recognition
  demo_apps:
  - action_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  description: 'The "i3d-rgb-tf" is a model for video classification, based on paper
    "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset" <https://arxiv.org/abs/1705.07750>.
    This model use RGB input stream and trained on Kinetics-400 dataset. Additionally,
    this model has initialize values from Inception v1 model pre-trained on ImageNet
    dataset.

    Originally redistributed as a checkpoint file, was converted to frozen graph.'
  license: https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/LICENSE
  name: i3d-rgb-tf
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/i3d-rgb-tf
  source: public
  task_type: action_recognition
  tf_devices: CPU
icnet-camvid-ava-sparse-30-0001:
  Source framework: TensorFlow\*
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: ICNet trained on CamVid in TensorFlow with 30% weight sparsity (70.08%
    mIoU)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  mIoU: 75.87%
  name: icnet-camvid-ava-sparse-30-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/icnet-camvid-ava-sparse-30-0001
  source: intel
  task_type: semantic_segmentation
icnet-camvid-ava-sparse-60-0001:
  Source framework: TensorFlow\*
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: ICNet trained on CamVid in TensorFlow with 60% weight sparsity (69.49%
    mIoU)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  mIoU: 75.79%
  name: icnet-camvid-ava-sparse-60-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/icnet-camvid-ava-sparse-60-0001
  source: intel
  task_type: semantic_segmentation
image-retrieval-0001:
  Source framework: TensorFlow\*
  Top1 accuracy: '0.834'
  demo_apps:
  - image_retrieval_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_retrieval_demo/python
  description: An image retrieval model based on Mobilenet v2 as a backbone. The model
    produces l2-normalized embeddings with dimension equals to 256.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: image-retrieval-0001
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/image-retrieval-0001
  source: intel
  task_type: object_attributes
inception-resnet-v2-tf:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "inception-resnet-v2" model is one of the Inception family of models
    designed to perform image classification. For details about this family of models,
    check out the paper <https://arxiv.org/abs/1602.07261>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: inception-resnet-v2-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/inception-resnet-v2-tf
  source: public
  task_type: classification
  tf_devices: CPU
instance-segmentation-person-0007:
  COCO val2017 box AP (person): 35.7%
  COCO val2017 mask AP (person): 30.9%
  Max objects to detect: '10'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - background_subtraction_demo
  - instance_segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  description: Instance segmentation model for one class - person. PointRend based
    architecture with EfficientNet-B1 backbone and lightweight Bbox and Mask heads.
  labels:
  - person
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-person-0007
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-person-0007
  source: intel
  task_type: instance_segmentation
instance-segmentation-security-0002:
  COCO val2017 box AP (max height 768, max width 1024): 39.86%
  COCO val2017 box AP (max short side 768, max long side 1024): 40.8%
  COCO val2017 mask AP (max height 768, max width 1024): 36.44%
  COCO val2017 mask AP (max short side 768, max long side 1024): 36.9%
  Max objects to detect: '100'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - multi_camera_multi_target_tracking_demo
  - instance_segmentation_demo
  - whiteboard_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/whiteboard_inpainting_demo/python
  description: General purpose instance segmentation model. Mask R-CNN with ResNet50
    backbone -  FPN -  RPN -  detection and segmentation heads.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-security-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-security-0002
  source: intel
  task_type: instance_segmentation
instance-segmentation-security-0091:
  COCO val2017 box AP (max height 800, max width 1344): 43.55%
  COCO val2017 box AP (max short side 800, max long side 1344): 45.8%
  COCO val2017 mask AP (max height 800, max width 1344): 38.14%
  COCO val2017 mask AP (max short side 800, max long side 1344): 39.7%
  Max objects to detect: '100'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - multi_camera_multi_target_tracking_demo
  - instance_segmentation_demo
  - whiteboard_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/whiteboard_inpainting_demo/python
  description: General purpose instance segmentation model. Cascade Mask R-CNN with
    ResNet101 backbone and deformable convolution V2.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-security-0091
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-security-0091
  source: intel
  task_type: instance_segmentation
instance-segmentation-security-0228:
  COCO val2017 box AP: 38.85%
  COCO val2017 mask AP: 33.9%
  Max objects to detect: '100'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - multi_camera_multi_target_tracking_demo
  - instance_segmentation_demo
  - whiteboard_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/whiteboard_inpainting_demo/python
  description: General purpose instance segmentation model. Mask R-CNN with ResNet101
    backbone and light FPN -  RPN -  detection and segmentation heads.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-security-0228
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-security-0228
  source: intel
  task_type: instance_segmentation
instance-segmentation-security-1039:
  COCO val2017 box AP: 32.9%
  COCO val2017 mask AP: 28.6%
  Max objects to detect: '100'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - multi_camera_multi_target_tracking_demo
  - instance_segmentation_demo
  - whiteboard_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/whiteboard_inpainting_demo/python
  description: General purpose instance segmentation model. Mask R-CNN with EfficientNet-B2
    backbone and light FPN -  RPN -  detection and segmentation heads.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-security-1039
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-security-1039
  source: intel
  task_type: instance_segmentation
instance-segmentation-security-1040:
  COCO val2017 box AP: 35.0%
  COCO val2017 mask AP: 31.2%
  Max objects to detect: '100'
  Source framework: PyTorch\*
  demo_apps:
  - background_subtraction_demo
  - multi_camera_multi_target_tracking_demo
  - instance_segmentation_demo
  - whiteboard_inpainting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/whiteboard_inpainting_demo/python
  description: General purpose instance segmentation model. Mask R-CNN with EfficientNet-B2
    backbone and light FPN -  RPN -  detection and segmentation heads.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: instance-segmentation-security-1040
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/instance-segmentation-security-1040
  source: intel
  task_type: instance_segmentation
landmarks-regression-retail-0009:
  Face location requirements: Tight crop
  Mean Normed Error (on VGGFace2): '0.0705'
  Source framework: PyTorch\*
  demo_apps:
  - face_recognition_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/face_recognition_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Landmark's detection. Used in Smart Classroom. The model is identical
    to 0002 but trained on internal dataset with improved regression loss.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/landmarks-regression-retail-0009.json
  name: landmarks-regression-retail-0009
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/landmarks-regression-retail-0009
  source: intel
  task_type: object_attributes
levit-128s:
  GFLOPs: '0.6177'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'The "levit-128s" model is one of the LeViT models family: a hybrid
    neural network for fast inference image classification. The model is pre-trained
    on the ImageNet dataset. LeViT-128s model is a small LeViT variant that has 128
    channels on input of the transformer stage and 2, 3 and 4 number of pairs of Attention
    and MLP blocks at 1, 2 and 3 model stages respectively.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/rwightman/pytorch-image-models>
    and paper <https://arxiv.org/abs/2104.01136>.'
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  name: levit-128s
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/levit-128s
  source: public
  task_type: classification
license-plate-recognition-barrier-0001:
  Min plate width: 94 pixels
  Ratio of correct reads: 88.58%
  Rotation in-plane: "\xB110\u02DA"
  Rotation out-of-plane: "Yaw: \xB145\u02DA / Pitch: \xB145\u02DA"
  Source framework: Caffe\*
  demo_apps:
  - security_barrier_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  description: Chinese license plate recognition
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: license-plate-recognition-barrier-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/license-plate-recognition-barrier-0001
  source: intel
  task_type: optical_character_recognition
license-plate-recognition-barrier-0007:
  Min plate width: 94 pixels
  Ratio of correct reads: 98%
  Rotation in-plane: "\xB110\u02DA"
  Rotation out-of-plane: "Yaw: \xB145\u02DA / Pitch: \xB145\u02DA"
  Source framework: TensorFlow\*
  demo_apps:
  - security_barrier_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  description: This model uses a small-footprint network trained end-to-end to recognize
    Chinese license plates in traffic.
  license: https://raw.githubusercontent.com/opencv/training_toolbox_tensorflow/develop/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/license-plate-recognition-barrier-0007.json
  name: license-plate-recognition-barrier-0007
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/license-plate-recognition-barrier-0007
  source: public
  task_type: optical_character_recognition
  tf_devices: CPU
machine-translation-nar-de-en-0002:
  BLEU: 21.4 %
  GOps: '23.19'
  Source framework: PyTorch\*
  demo_apps:
  - machine_translation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/machine_translation_demo/python
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: machine-translation-nar-de-en-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/machine-translation-nar-de-en-0002
  source: intel
  task_type: machine_translation
machine-translation-nar-en-de-0002:
  BLEU: 17.7 %
  GOps: '23.19'
  Source framework: PyTorch\*
  demo_apps:
  - machine_translation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/machine_translation_demo/python
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: machine-translation-nar-en-de-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/machine-translation-nar-en-de-0002
  source: intel
  task_type: machine_translation
machine-translation-nar-en-ru-0002:
  BLEU: 22.7 %
  GOps: '23.17'
  Source framework: PyTorch\*
  demo_apps:
  - machine_translation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/machine_translation_demo/python
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: machine-translation-nar-en-ru-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/machine-translation-nar-en-ru-0002
  source: intel
  task_type: machine_translation
machine-translation-nar-ru-en-0002:
  BLEU: 23.1 %
  GOps: '23.17'
  Source framework: PyTorch\*
  demo_apps:
  - machine_translation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/machine_translation_demo/python
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: machine-translation-nar-ru-en-0002
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/machine-translation-nar-ru-en-0002
  source: intel
  task_type: machine_translation

midasnet:
  GFLOPs: '207.25144'
  Source framework: PyTorch\*
  Type: Monodepth
  demo_apps:
  - monodepth_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/monodepth_demo/python
  license: https://raw.githubusercontent.com/intel-isl/MiDaS/master/LICENSE
  name: midasnet
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/midasnet
  rmse: '0.07071'
  source: public
  task_type: monocular_depth_estimation
mixnet-l:
  GFLOPs: '0.565'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: MixNets are a family of mobile-sizes image classification models equipped
    with MixConv, a new type of mixed depthwise convolutions. There are three MixNet
    architectures - "MixNet-S" (Small), "MixNet-M" (Middle), "MixNet-L" (Large). The
    main differences are using MixConv with different kernel sizes and number of layers.
    Using "MixNet-L" allows to achieve greater accuracy. Use this link <https://arxiv.org/abs/1907.09595>
    to learn more about MixNet architectures. "MixNet-L" was pretrained in TensorFlow*.
    All the MixNet models have been pretrained on the ImageNet* image database. For
    details about this family of models, check out the TensorFlow Cloud TPU repository
    <https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/tensorflow/tpu/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mixnet-l
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mixnet-l
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v1-0.25-128:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v1-0.25-128" is one of MobileNets - small, low-latency,
    low-power models parameterized to meet the resource constraints of a variety of
    use cases. They can be built upon for classification, detection, embeddings and
    segmentation similar to how other popular large scale models are used. For details,
    see paper <https://arxiv.org/abs/1704.04861>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v1-0.25-128
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v1-0.25-128
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v1-1.0-224-tf:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v1-1.0-224" is one of MobileNets - small, low-latency,
    low-power models parameterized to meet the resource constraints of a variety of
    use cases. They can be built upon for classification, detection, embeddings and
    segmentation similar to how other popular large scale models are used. For details,
    see the paper <https://arxiv.org/abs/1704.04861>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v1-1.0-224-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v1-1.0-224-tf
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v2-1.0-224:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v2-1.0-224" is one of MobileNet models, which are small,
    low-latency, low-power, and parameterized to meet the resource constraints of
    a variety of use cases. They can be used for classification, detection, embeddings,
    and segmentation like other popular large-scale models. For details, see the paper
    <https://arxiv.org/abs/1704.04861>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v2-1.0-224
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v2-1.0-224
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v2-1.4-224:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v2-1.4-224" is one of MobileNets - small, low-latency,
    low-power models parameterized to meet the resource constraints of a variety of
    use cases. They can be built upon for classification, detection, embeddings and
    segmentation similar to how other popular large scale models are used. For details,
    see the paper <https://arxiv.org/abs/1704.04861>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v2-1.4-224
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v2-1.4-224
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v2-pytorch:
  GFLOPs: '0.615'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'MobileNet V2 is image classification model pre-trained on ImageNet
    dataset. This is a PyTorch* implementation of MobileNetV2 architecture as described
    in the paper "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,
    Detection and Segmentation" <https://arxiv.org/abs/1801.04381>.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v2-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v2-pytorch
  source: public
  task_type: classification
mobilenet-v3-large-1.0-224-tf:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v3-large-1.0-224-tf" is one of MobileNets V3 - next generation
    of MobileNets, based on a combination of complementary search techniques as well
    as a novel architecture design. "mobilenet-v3-large-1.0-224-tf" is targeted for
    high resource use cases. For details see paper <https://arxiv.org/abs/1905.02244>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v3-large-1.0-224-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v3-large-1.0-224-tf
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-v3-small-1.0-224-tf:
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"mobilenet-v3-small-1.0-224-tf" is one of MobileNets V3 - next generation
    of MobileNets, based on a combination of complementary search techniques as well
    as a novel architecture design. "mobilenet-v3-small-1.0-224-tf" is targeted for
    low resource use cases. For details see paper <https://arxiv.org/abs/1905.02244>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: mobilenet-v3-small-1.0-224-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-v3-small-1.0-224-tf
  source: public
  task_type: classification
  tf_devices: CPU
mobilenet-yolo-v4-syg:
  GFLOPs: '65.984'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: This is a Keras\* version of `mobilenet-yolov4` model designed to perform
    real-time vehicle detection. The weights are pretrained by BDD100k and retrained
    by our own dataset. For details see repository <https://github.com/legendary111/mobilenet-yolo-v4-syg/>,
    paper of MobileNetV2<https://arxiv.org/abs/1801.04381> and YOLOv4<https://arxiv.org/abs/2004.10934>
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 86.35%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/blob/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/mobilenet-yolo-v4-syg.json
  name: mobilenet-yolo-v4-syg
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mobilenet-yolo-v4-syg
  source: public
  task_type: detection
  tf_devices: CPU
modnet-photographic-portrait-matting:
  MAD: '5.21'
  MSE: '727.95'
  Source framework: PyTorch\*
  Type: Background Matting
  demo_apps:
  - background_subtraction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  description: 'The "modnet-photographic-portrait-matting" model is a lightweight
    matting objective decomposition network (MODNet) for photographic portrait matting
    in real-time with a single input image with MobileNetV2 backbone. The model is
    pre-trained in PyTorch* framework and converted to ONNX* format.

    More details provided in the paper <https://arxiv.org/abs/2011.11961> and repository
    <https://github.com/ZHKKKe/MODNet>.'
  license: https://raw.githubusercontent.com/ZHKKKe/MODNet/master/LICENSE
  name: modnet-photographic-portrait-matting
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/modnet-photographic-portrait-matting
  source: public
  task_type: background_matting
modnet-webcam-portrait-matting:
  MAD: '5.66'
  MSE: '762.52'
  Source framework: PyTorch\*
  Type: Background Matting
  demo_apps:
  - background_subtraction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  description: 'The "modnet-webcam-portrait-matting" model is a lightweight matting
    objective decomposition network (MODNet) for online video portrait matting in
    real-time with a single input image with MobileNetV2 backbone. The model is pre-trained
    in PyTorch* framework and converted to ONNX* format.

    More details provided in the paper <https://arxiv.org/abs/2011.11961> and repository
    <https://github.com/ZHKKKe/MODNet>.'
  license: https://raw.githubusercontent.com/ZHKKKe/MODNet/master/LICENSE
  name: modnet-webcam-portrait-matting
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/modnet-webcam-portrait-matting
  source: public
  task_type: background_matting
mozilla-deepspeech-0.6.1:
  GFlops per audio frame: '0.0472'
  GFlops per second of audio: '2.36'
  Source framework: TensorFlow\*
  Type: Speech recognition
  WER @ Librispeech test-clean: '**7.55%**'
  demo_apps:
  - speech_recognition_deepspeech_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/speech_recognition_deepspeech_demo/python
  description: 'The "mozilla-deepspeech-0.6.1" model is a speech recognition neural
    network pre-trained by Mozilla based on DeepSpeech architecture (CTC decoder with
    beam search and n-gram language model) with changed neural network topology.

    For details on the original DeepSpeech, see paper <https://arxiv.org/abs/1412.5567>.

    For details on this model, see repository <https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.1>.'
  license: https://raw.githubusercontent.com/mozilla/DeepSpeech/master/LICENSE
  name: mozilla-deepspeech-0.6.1
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mozilla-deepspeech-0.6.1
  source: public
  task_type: speech_recognition
  tf_devices: CPU
mozilla-deepspeech-0.8.2:
  GFlops per audio frame: '0.0472'
  GFlops per second of audio: '2.36'
  Source framework: TensorFlow\*
  Type: Speech recognition
  WER @ LibriSpeech test-clean: '**6.15%**'
  demo_apps:
  - speech_recognition_deepspeech_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/speech_recognition_deepspeech_demo/python
  description: 'The "mozilla-deepspeech-0.8.2" model is a speech recognition neural
    network pre-trained by Mozilla based on DeepSpeech architecture (CTC decoder with
    beam search and n-gram language model) with changed neural network topology.

    For details on the original DeepSpeech, see paper <https://arxiv.org/abs/1412.5567>.

    For details on this model, see repository <https://github.com/mozilla/DeepSpeech/releases/tag/v0.8.2>.'
  license: https://raw.githubusercontent.com/mozilla/DeepSpeech/master/LICENSE
  name: mozilla-deepspeech-0.8.2
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/mozilla-deepspeech-0.8.2
  source: public
  task_type: speech_recognition
  tf_devices: CPU
nanodet-m-1.5x-416:
  GFLOPs: '2.3895'
  Source framework: PyTorch\*
  Type: Object detection
  coco_orig_precision: 27.38%
  coco_precision: 26.63%
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'The "nanodet-m-1.5x-416" model is one from NanoDet models family,
    which is a FCOS-style one-stage anchor-free object detection model which using
    Generalized Focal Loss as classification and regression loss. The model is a super
    fast and high accuracy lightweight model with ShuffleNetV2 1.5x backbone. This
    model was pre-trained on Common Objects in Context (COCO) <https://cocodataset.org/#home>
    dataset.

    More details provided in the repository <https://github.com/RangiLyu/nanodet>.'
  license: https://raw.githubusercontent.com/RangiLyu/nanodet/main/LICENSE
  name: nanodet-m-1.5x-416
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/nanodet-m-1.5x-416
  source: public
  task_type: detection
nanodet-plus-m-1.5x-416:
  GFLOPs: '3.0147'
  Source framework: PyTorch\*
  Type: Object detection
  coco_orig_precision: 33.77%
  coco_precision: 34.53%
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'The "nanodet-plus-m-1.5x-416" model is one from NanoDet models family,
    which is a FCOS-style one-stage anchor-free object detection model which using
    Generalized Focal Loss as classification and regression loss. A novel label assignment
    strategy with a simple assign guidance module (AGM) and a dynamic soft label assigner
    (DSLA) is used in NanoDet-Plus to solve the optimal label assignment problem in
    lightweight model training. Also a light feature pyramid called Ghost-PAN is introduced
    in Plus models to enhance multi-layer feature fusion. The model is a super fast
    and high accuracy lightweight model with ShuffleNetV2 1.5x backbone. This model
    was pre-trained on Common Objects in Context (COCO) <https://cocodataset.org/#home>
    dataset.

    More details provided in the repository <https://github.com/RangiLyu/nanodet>.'
  license: https://raw.githubusercontent.com/RangiLyu/nanodet/main/LICENSE
  name: nanodet-plus-m-1.5x-416
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/nanodet-plus-m-1.5x-416
  source: public
  task_type: detection
netvlad-tf:
  GFLOPs: '36.6374'
  Source framework: TensorFlow\*
  Type: Place recognition
  demo_apps:
  - place_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/place_recognition_demo/python
  description: 'NetVLAD is a CNN architecture which tackles the problem of large scale
    visual place recognition. The architecture uses VGG 16 as base network and NetVLAD
    - a new trainable generalized VLAD (Vector of Locally Aggregated Descriptors)
    layer. It is a place recognition model pre-trained on the Pittsburgh 250k <http://www.ok.ctrl.titech.ac.jp/~torii/project/repttile/>
    dataset.

    For details see repository <https://github.com/uzh-rpg/netvlad_tf_open> and paper
    <https://arxiv.org/abs/1511.07247>.'
  license: https://raw.githubusercontent.com/uzh-rpg/netvlad_tf_open/master/LICENSE
  localization_recall: 82.0321%
  name: netvlad-tf
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/netvlad-tf
  source: public
  task_type: place_recognition
  tf_devices: CPU
nfnet-f0:
  GFLOPs: '24.8053'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'NFNet F0 is one of the image classification Normalizer-Free models
    pre-trained on the ImageNet dataset. NFNets are Normalizer-Free ResNets in which
    use Adaptive Gradient Clipping (AGC), which clips gradients based on the unit-wise
    ratio of gradient norms to parameter norms.

    F0 variant is the baseline variant with a depth pattern [1, 2, 6, 3] (indicating
    how many bottleneck blocks to allocate to each stage). Each subsequent variant
    has this depth pattern multiplied by N (where N = 1 for F0).

    The model input is a blob that consists of a single image of "1, 3, 256, 256"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/rwightman/pytorch-image-models>
    and paper <https://arxiv.org/abs/2102.06171>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: nfnet-f0
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/nfnet-f0
  source: public
  task_type: classification
noise-suppression-denseunet-ll-0001:
  GOps: '0.2'
  SISDR for input noisy signal: 11.7    dB
  SISDR for output cleaned signal: 20.0    dB
  SISDR increase: +8.3    dB
  Source framework: PyTorch\*
  demo_apps:
  - noise_suppression_demo
  - noise_suppression_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/noise_suppression_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/noise_suppression_demo/python
  description: Model to suppress noise and keep speech with low latency (delay<40ms).
    The model architecture looks like UNet with DenseNet blocks inside. The model
    is trained on DNS-chalange datasets https://github.com/microsoft/DNS-Challenge/blob/master/README.md.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: noise-suppression-denseunet-ll-0001
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/noise-suppression-denseunet-ll-0001
  source: intel
  task_type: noise_suppression
noise-suppression-poconetlike-0001:
  GOps: '1.2'
  SISDR for input noisy signal: 11.73   dB
  SISDR for output cleaned signal: 20.54   dB
  SISDR increase: +8.81   dB
  Source framework: PyTorch\*
  demo_apps:
  - noise_suppression_demo
  - noise_suppression_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/noise_suppression_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/noise_suppression_demo/python
  description: PoCoNet-like model to suppress noise and keep speech. The model is
    trained on DNS-chalange datasets https://github.com/microsoft/DNS-Challenge/blob/master/README.md.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: noise-suppression-poconetlike-0001
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/noise-suppression-poconetlike-0001
  source: intel
  task_type: noise_suppression
open-closed-eye-0001:
  Source framework: PyTorch\*
  demo_apps:
  - gaze_estimation_demo
  - gaze_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gaze_estimation_demo/cpp
  description: Fully convolutional network for recognition of eye state ('open', 'closed').
  license: https://raw.githubusercontent.com/opencv/openvino_training_extensions/develop/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/open-closed-eye-0001.json
  name: open-closed-eye-0001
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/open-closed-eye-0001
  source: public
  task_type: classification
pedestrian-and-vehicle-detector-adas-0001:
  AP for pedestrians: 88%
  AP for vehicles: 90%
  GFLOPS: '3.974'
  Source framework: Caffe\*
  Target pedestrian size: 60x120 pixels
  Target vehicle size: 40x30 pixels
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: Pedestrian and Vehicle detector based on ssd + mobilenet with reduced
    channels number.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/pedestrian-and-vehicle-detector-adas-0001.json
  name: pedestrian-and-vehicle-detector-adas-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/pedestrian-and-vehicle-detector-adas-0001
  source: intel
  task_type: detection
pedestrian-detection-adas-0002:
  Average Precision (AP): 88%
  Max objects to detect: '200'
  Source framework: Caffe\*
  Target pedestrian size: 60 x 120 pixels on Full HD image
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: Pedestrian detector based on ssd + mobilenet with reduced channels
    number.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/pedestrian-detection-adas-0002.json
  name: pedestrian-detection-adas-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/pedestrian-detection-adas-0002
  source: intel
  task_type: detection
person-attributes-recognition-crossroad-0230:
  Attribute: F1
  Min object width: 80 pixels
  Occlusion coverage: <20%
  Pedestrian pose: Standing person
  Source framework: PyTorch\*
  Supported attributes: is_male, has_bag, has_backpack, has hat, has longsleeves,
    has longpants, has longhair, has coat_jacket
  '`has_backpack`': '0.77'
  '`has_bag`': '0.66'
  '`has_coat_jacket`': NA
  '`has_hat`': '0.64'
  '`has_longhair`': '0.83'
  '`has_longpants`': '0.83'
  '`has_longsleeves`': '0.21'
  '`is_male`': '0.91'
  demo_apps:
  - crossroad_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  description: Pedestrian attributes recognition based on a PVANet with hyperfeatures
    backbone + classification head
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-attributes-recognition-crossroad-0230.json
  name: person-attributes-recognition-crossroad-0230
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-attributes-recognition-crossroad-0230
  source: intel
  task_type: object_attributes
person-attributes-recognition-crossroad-0234:
  Attribute: F1
  Min object width: 80 pixels
  Occlusion coverage: <20%
  Pedestrian pose: Standing person
  Source framework: PyTorch\*
  Supported attributes: '`is_male`, `has_bag`, `has_hat`, `has_longsleeves`, `has_longpants`,
    `has_longhair`, `has_coat_jacket`'
  '`has_bag`': '0.44'
  '`has_coat_jacket`': NA
  '`has_hat`': '0.74'
  '`has_longhair`': '0.84'
  '`has_longpants`': '0.89'
  '`has_longsleeves`': '0.45'
  '`is_male`': '0.92'
  demo_apps:
  - crossroad_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  description: Pedestrian attributes recognition based on ResNet-50 backbone
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-attributes-recognition-crossroad-0234.json
  name: person-attributes-recognition-crossroad-0234
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-attributes-recognition-crossroad-0234
  source: intel
  task_type: object_attributes
person-attributes-recognition-crossroad-0238:
  Attribute: F1
  Min object width: 80 pixels
  Occlusion coverage: <20%
  Pedestrian pose: Standing person
  Source framework: PyTorch\*
  Supported attributes: '`is_male`, `has_bag`, `has_hat`, `has_longsleeves`, `has_longpants`,
    `has_longhair`, `has_coat_jacket`'
  '`has_bag`': '0.48'
  '`has_coat_jacket`': NA
  '`has_hat`': '0.42'
  '`has_longhair`': '0.77'
  '`has_longpants`': '0.75'
  '`has_longsleeves`': '0.17'
  '`is_male`': '0.80'
  demo_apps:
  - crossroad_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  description: Pedestrian attributes recognition based on Inception V3 backbone
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-attributes-recognition-crossroad-0238.json
  name: person-attributes-recognition-crossroad-0238
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-attributes-recognition-crossroad-0238
  source: intel
  task_type: object_attributes
person-detection-0106:
  AP @ [ IoU=0.50:0.95 ]: 0.442 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person detector based on Cascade R-CNN with ResNet50 backbone
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-0106
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0106
  source: intel
  task_type: detection
person-detection-0200:
  AP @ [ IoU=0.50:0.95 ]: 0.2398 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 256x256 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-0200.json
  name: person-detection-0200
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0200
  source: intel
  task_type: detection
person-detection-0201:
  AP @ [ IoU=0.50:0.95 ]: 0.2975 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 384x384 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-0201.json
  name: person-detection-0201
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0201
  source: intel
  task_type: detection
person-detection-0202:
  AP @ [ IoU=0.50:0.95 ]: 0.322 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 512x512 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-0202.json
  name: person-detection-0202
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0202
  source: intel
  task_type: detection
person-detection-0203:
  AP @ [ IoU=0.50:0.95 ]: 0.408 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Detection based on MobileNetV2 (ATSS) on 864x480 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-0203.json
  name: person-detection-0203
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0203
  source: intel
  task_type: detection
person-detection-0301:
  AP @ [ IoU=0.50:0.95 ]: 0.439 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Detection based on Resnet50 (VFNet) on 1344x800 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-0301
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0301
  source: intel
  task_type: detection
person-detection-0302:
  AP @ [ IoU=0.50:0.95 ]: 0.447 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Detection based on Resnet50 (ATSS) on 1280x720 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-0302
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0302
  source: intel
  task_type: detection
person-detection-0303:
  AP @ [ IoU=0.50:0.95 ]: 0.444 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Detection based on MobileNetV2 (ATSS) on 1280x720 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-0303
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-0303
  source: intel
  task_type: detection
person-detection-action-recognition-0005:
  Accuracy (internal test set 2): 83.8%
  Detector AP (internal test set 2): 80.0%
  Min pedestrian height: 80 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: Sitting, standing, raising hand
  Source framework: Caffe\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Second generation of action detection (SSD-based) model to use in Smart
    Classroom.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-action-recognition-0005
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-action-recognition-0005
  source: intel
  task_type: detection
person-detection-action-recognition-0006:
  ? ''
  : turned around, lie on the desk
  Accuracy (internal test set 2): 80.74%
  Detector AP (internal test set 2): 90.70%
  Min pedestrian height: 80 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: sitting, writing, raising_hand, standing,
  Source framework: TensorFlow\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Person Detection and Action Recognition model on TF
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-action-recognition-0006
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-action-recognition-0006
  source: intel
  task_type: detection
person-detection-action-recognition-teacher-0002:
  Accuracy (internal test set 1): 72.4%
  Detector AP (internal test set 2): 80.0%
  Min pedestrian height: 80 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: Standing, writing, demonstrating
  Source framework: Caffe\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - smart_classroom_demo
  - smart_classroom_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Second generation of action detection (SSD-based) model to use in Smart
    Classroom for classification teacher's actions.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-action-recognition-teacher-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-action-recognition-teacher-0002
  source: intel
  task_type: detection
person-detection-asl-0001:
  Minimal person height: 100 pixel
  Persons AP on COCO: 79.35%
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - gesture_recognition_demo
  - gesture_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/gesture_recognition_demo/python
  description: Person detector (ShuffleNetv2 backbone and FCOS head) for ASL scenario
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-0203.json
  name: person-detection-asl-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-asl-0001
  source: intel
  task_type: detection
person-detection-raisinghand-recognition-0001:
  Accuracy (internal test set 2): 90.5%
  Detector AP (internal test set 2): 80.0%
  Min pedestrian height: 80 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: Sitting, standing, raising hand
  Source framework: Caffe\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - smart_classroom_demo
  - smart_classroom_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smart_classroom_demo/cpp
  description: Raising-hand action detection (SSD-based) model to use in Smart Classroom
    environment.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-raisinghand-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-raisinghand-recognition-0001
  source: intel
  task_type: detection
person-detection-retail-0002:
  AP: 80.14%
  Max objects to detect: '200'
  Min pedestrian height: 80 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: Caffe\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person detection (HyperNet+RFCN+DetectionOutput). Used in Audience
    Analytics.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-detection-retail-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-retail-0002
  source: intel
  task_type: detection
person-detection-retail-0013:
  AP: 88.62%
  Min pedestrian height: 100 pixels (on 1080p)
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: Caffe\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - pedestrian_tracker_demo
  - multi_camera_multi_target_tracking_demo
  - single_human_pose_estimation_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: Pedestrian detection (RMNet with lrelu + SSD)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-detection-retail-0013.json
  name: person-detection-retail-0013
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-detection-retail-0013
  source: intel
  task_type: detection
person-reidentification-retail-0277:
  Market-1501 mAP: 87.7 %
  Market-1501 rank@1 accuracy: 96.2 %
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: PyTorch\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - pedestrian_tracker_demo
  - multi_camera_multi_target_tracking_demo
  - crossroad_camera_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: High accuracy single embedding-based person reidentification model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-reidentification-retail-0277
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-reidentification-retail-0277
  source: intel
  task_type: object_attributes
person-reidentification-retail-0286:
  Market-1501 mAP: 83.7 %
  Market-1501 rank@1 accuracy: 94.8 %
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: PyTorch\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - pedestrian_tracker_demo
  - multi_camera_multi_target_tracking_demo
  - crossroad_camera_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: Single embedding-based person reidentification model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-reidentification-retail-0286
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-reidentification-retail-0286
  source: intel
  task_type: object_attributes
person-reidentification-retail-0287:
  Market-1501 mAP: 76.6 %
  Market-1501 rank@1 accuracy: 92.9 %
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: PyTorch\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - pedestrian_tracker_demo
  - multi_camera_multi_target_tracking_demo
  - crossroad_camera_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: Single embedding-based person reidentification model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-reidentification-retail-0287
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-reidentification-retail-0287
  source: intel
  task_type: object_attributes
person-reidentification-retail-0288:
  Market-1501 mAP: 58.857 %
  Market-1501 rank@1 accuracy: 86.1 %
  Occlusion coverage: <50%
  Pose coverage: Standing upright, parallel to image plane
  Source framework: PyTorch\*
  Support of occluded pedestrians: 'YES'
  demo_apps:
  - pedestrian_tracker_demo
  - multi_camera_multi_target_tracking_demo
  - crossroad_camera_demo
  - social_distance_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/social_distance_demo/cpp
  description: Single embedding-based person reidentification model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: person-reidentification-retail-0288
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-reidentification-retail-0288
  source: intel
  task_type: object_attributes
person-vehicle-bike-detection-2000:
  AP @ [ IoU=0.50:0.95 ]: 0.1647 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 256x256 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-2000.json
  name: person-vehicle-bike-detection-2000
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-2000
  source: intel
  task_type: detection
person-vehicle-bike-detection-2001:
  AP @ [ IoU=0.50:0.95 ]: 0.2259 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 384x484 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-2001.json
  name: person-vehicle-bike-detection-2001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-2001
  source: intel
  task_type: detection
person-vehicle-bike-detection-2002:
  AP @ [ IoU=0.50:0.95 ]: 0.2459 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: MobileNetV2-SSD with two heads and clustered priors for 512x512 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-2002.json
  name: person-vehicle-bike-detection-2002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-2002
  source: intel
  task_type: detection
person-vehicle-bike-detection-2003:
  AP @ [ IoU=0.50:0.95 ]: 0.336  (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Vehicle Bike Detection based on MobileNetV2 (ATSS) on 864x480
    resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-2003.json
  name: person-vehicle-bike-detection-2003
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-2003
  source: intel
  task_type: detection
person-vehicle-bike-detection-2004:
  AP @ [ IoU=0.50:0.95 ]: 0.274 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Person Vehicle Bike Detection based on MobileNetV2 (ATSS) on 448x256
    resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-2004.json
  name: person-vehicle-bike-detection-2004
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-2004
  source: intel
  task_type: detection
person-vehicle-bike-detection-crossroad-0078:
  AP bikes: 44.14%
  AP people: 77.47%
  AP vehicles: 74.94%
  Bike: 55,692
  Max objects to detect: '200'
  Mean Average Precision (mAP): 65.12%
  Pedestrian: 706,786
  Source framework: Caffe\*
  Type of object: Number of bounding boxes
  Vehicle: 501,548
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - crossroad_camera_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: Multiclass (person -  vehicle -  non-vehicle) detector based on SSD
    detection architecture -  RMNet backbone and learnable image downscale block (person-vehicle-bike-detection-crossroad-0066
    with extra pooling)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-crossroad-0078.json
  name: person-vehicle-bike-detection-crossroad-0078
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-crossroad-0078
  source: intel
  task_type: detection
person-vehicle-bike-detection-crossroad-1016:
  AP bikes: 36.18%
  AP people: 73.63%
  AP vehicles: 77.84%
  Max objects to detect: '200'
  Mean Average Precision (mAP): 62.55%
  Non-vehicle: 62,334
  Pedestrian: 1,114,799
  Source framework: PyTorch\*
  Type of object: Number of bounding boxes
  Vehicle: 810,323
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - crossroad_camera_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: Multiclass (person -  vehicle -  non-vehicle) detector based on SSD
    detection architecture -  MobileNetV2 backbone)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-crossroad-1016.json
  name: person-vehicle-bike-detection-crossroad-1016
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-crossroad-1016
  source: intel
  task_type: detection
person-vehicle-bike-detection-crossroad-yolov3-1020:
  AP bikes/motorcycles: 25.66%
  AP people: 58.94%
  AP vehicles: 62.05%
  Bike/Motorcycle: '30220'
  Mean Average Precision (mAP): 48.89%
  Pedestrian: '119546'
  Source framework: Keras\*
  Type of object: Number of bounding boxes
  Vehicle: '121111'
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - pedestrian_tracker_demo
  - multi_channel_object_detection_demo_yolov3
  - crossroad_camera_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_object_detection_demo_yolov3/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/crossroad_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: YOLO v3 finetuned for person-vehicle-bike detection task
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/person-vehicle-bike-detection-crossroad-yolov3-1020.json
  name: person-vehicle-bike-detection-crossroad-yolov3-1020
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/person-vehicle-bike-detection-crossroad-yolov3-1020
  source: intel
  task_type: detection
product-detection-0001:
  Average Precision (AP) @[ IoU=0.50:0.95,  area=all, maxDets=100 ]: '0.715'
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Product detection based on MobileNetV2.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/product-detection-0001.json
  name: product-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/product-detection-0001
  source: intel
  task_type: detection
pspnet-pytorch:
  Source framework: PyTorch\*
  Type: Semantic segmentation
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: '"pspnet-pytorch" is a semantic segmentation model, pre-trained on
    Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/> dataset for 21 object classes,
    listed in "<omz_dir>/data/dataset_classes/voc_20cl_bkgr.txt" file. The model was
    built on ResNetV1-50 <https://arxiv.org/pdf/1812.01187.pdf> backbone and PSP segmentation
    head. This model is used for pixel-level prediction tasks. For details see repository
    <https://github.com/open-mmlab/mmsegmentation/tree/master>, paper <https://arxiv.org/abs/1612.01105>.'
  license: https://raw.githubusercontent.com/open-mmlab/mmsegmentation/master/LICENSE
  mean_iou: 70.1%
  name: pspnet-pytorch
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/pspnet-pytorch
  source: public
  task_type: semantic_segmentation
quartznet-15x5-en:
  GFLOPs: '2.4195'
  Source framework: PyTorch\*
  Type: Speech recognition
  WER @ Librispeech test-clean: 3.86%
  demo_apps:
  - speech_recognition_quartznet_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/speech_recognition_quartznet_demo/python
  license: https://raw.githubusercontent.com/NVIDIA/NeMo/main/LICENSE
  name: quartznet-15x5-en
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/quartznet-15x5-en
  source: public
  task_type: speech_recognition
regnetx-3.2gf:
  GFLOPs: '6.3893'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: The "regnetx-3.2gf" model is one of the RegNetX design space <https://arxiv.org/pdf/2003.13678>
    models designed to perform image classification. The RegNet design space provides
    simple and fast networks that work well across a wide range of flop regimes. This
    model was pre-trained in PyTorch*. All RegNet classification models have been
    pre-trained on the ImageNet dataset. For details about this family of models,
    check out the Codebase for Image Classification Research <https://github.com/facebookresearch/pycls>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/facebookresearch/pycls/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: regnetx-3.2gf
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/regnetx-3.2gf
  source: public
  task_type: classification
repvgg-a0:
  GFLOPs: '2.7286'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'RepVGG-A0 is one of the RepVGG image classification models pre-trained
    on ImageNet dataset. RepVGG is architecture of convolutional neural network, which
    has a VGG-like inference-time body and a structural re-parameterization technique.
    The 3x3 layers are arranged into five stages. RepVGG-A stages have 1, 2, 4, 14,
    1 layers respectively. The layer width for these models is determined by uniform
    scaling the classic width setting of [64a, 128a, 256a, 512b]. RepVGG-A0 model
    has multipliers a = 0.75 and b = 2.5.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/DingXiaoH/RepVGG> and paper <https://arxiv.org/abs/2101.03697>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/DingXiaoH/RepVGG/main/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: repvgg-a0
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/repvgg-a0
  source: public
  task_type: classification
repvgg-b1:
  GFLOPs: '23.6472'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'RepVGG-B1 is one of the RepVGG image classification models pre-trained
    on ImageNet dataset. RepVGG is architecture of convolutional neural network, which
    has a VGG-like inference-time body and a structural re-parameterization technique.
    The 3x3 layers are arranged into five stages. RepVGG-B stages have 1, 4, 6, 16,
    1 layers respectively. The layer width for these models is determined by uniform
    scaling the classic width setting of [64a, 128a, 256a, 512b]. RepVGG-B1 model
    has multipliers a = 2 and b = 4.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/DingXiaoH/RepVGG> and paper <https://arxiv.org/abs/2101.03697>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/DingXiaoH/RepVGG/main/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: repvgg-b1
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/repvgg-b1
  source: public
  task_type: classification
repvgg-b3:
  GFLOPs: '52.4407'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'RepVGG-B3 is a heavyweight RepVGG image classification model pre-trained
    on ImageNet dataset in 200 epochs. RepVGG is architecture of convolutional neural
    network, which has a VGG-like inference-time body and a structural re-parameterization
    technique. The 3x3 layers are arranged into five stages. RepVGG-B stages have
    1, 4, 6, 16, 1 layers respectively. The layer width for these models is determined
    by uniform scaling the classic width setting of [64a, 128a, 256a, 512b]. RepVGG-B3
    model has multipliers a = 3 and b = 5.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.

    For details see repository <https://github.com/DingXiaoH/RepVGG> and paper <https://arxiv.org/abs/2101.03697>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/DingXiaoH/RepVGG/main/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: repvgg-b3
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/repvgg-b3
  source: public
  task_type: classification
resnest-50-pytorch:
  GFLOPs: '10.8148'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'ResNeSt-50 is image classification model pre-trained on ImageNet dataset.
    ResNeSt is stacked in ResNet-style from modular Split-Attention blocks that enables
    attention across feature-map groups.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications  matching
    with those in the ImageNet database.

    For details see repository <https://github.com/zhanghang1989/ResNeSt> and paper
    <https://arxiv.org/abs/2004.08955>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/zhanghang1989/ResNeSt/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: resnest-50-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/resnest-50-pytorch
  source: public
  task_type: classification
resnet-18-pytorch:
  GFLOPs: '3.637'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'ResNet 18 is image classification model pre-trained on ImageNet dataset.
    This is PyTorch* implementation based on architecture described in paper "Deep
    Residual Learning for Image Recognition" <https://arxiv.org/abs/1512.03385> in
    TorchVision package (see here <https://github.com/pytorch/vision>).

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: resnet-18-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/resnet-18-pytorch
  source: public
  task_type: classification
resnet-34-pytorch:
  GFLOPs: '7.3409'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'ResNet 34 is image classification model pre-trained on ImageNet dataset.
    This is PyTorch* implementation based on architecture described in paper "Deep
    Residual Learning for Image Recognition" <https://arxiv.org/abs/1512.03385> in
    TorchVision package (see here <https://github.com/pytorch/vision>).

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: resnet-34-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/resnet-34-pytorch
  source: public
  task_type: classification
resnet-50-pytorch:
  GFLOPs: '8.216'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'ResNet 50 is image classification model pre-trained on ImageNet dataset.
    This is PyTorch* implementation based on architecture described in paper "Deep
    Residual Learning for Image Recognition" <https://arxiv.org/abs/1512.03385> in
    TorchVision package (see here <https://github.com/pytorch/vision>).

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: resnet-50-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/resnet-50-pytorch
  source: public
  task_type: classification
resnet-50-tf:
  GFLOPs: '8.2164'
  Source framework: TensorFlow\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: '"resnet-50-tf" is a TensorFlow* implementation of ResNet-50 - an image
    classification model pre-trained on the ImageNet dataset. Originally redistributed
    in Saved model format, converted to frozen graph using "tf.graph_util" module.
    For details see paper <https://arxiv.org/abs/1512.03385>, repository <https://github.com/tensorflow/models/tree/v2.2.0/official/r1/resnet>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: resnet-50-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/resnet-50-tf
  source: public
  task_type: classification
  tf_devices: CPU
resnet18-xnor-binary-onnx-0001:
  Accuracy top-1 (ImageNet): 61.71%
  Image size: 224x224
  Source framework: PyTorch\*
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: ResNet-18 Binary with XNOR weight binarization
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/resnet18-xnor-binary-onnx-0001.json
  name: resnet18-xnor-binary-onnx-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/resnet18-xnor-binary-onnx-0001
  source: intel
  task_type: classification
resnet50-binary-0001:
  Accuracy top-1 (ImageNet): 70.69%
  Image size: 224x224
  Source framework: PyTorch\*
  bin conv  MI1ops: '7218'
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: ResNet-50 Binary
  fp32 conv MFlops: '960'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/resnet50-binary-0001.json
  name: resnet50-binary-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/resnet50-binary-0001
  source: intel
  task_type: classification
retinaface-resnet50-pytorch:
  AP ([WIDER](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)): 91.78%
  GFLOPs: '88.8627'
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: The "retinaface-resnet50-pytorch" model is a PyTorch* implementation
    of medium size RetinaFace model with ResNet50 backbone for Face Localization.
    It can output face bounding boxes and five facial landmarks in a single forward
    pass. More details provided in the paper <https://arxiv.org/abs/1905.00641> and
    repository <https://github.com/biubug6/Pytorch_Retinaface>
  license: https://raw.githubusercontent.com/biubug6/Pytorch_Retinaface/master/LICENSE.MIT
  name: retinaface-resnet50-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/retinaface-resnet50-pytorch
  source: public
  task_type: detection
retinanet-tf:
  Source framework: TensorFlow\*
  Type: Object detection
  coco_precision: 33.15%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: RetinaNet is the dense object detection model with ResNet50 backbone,
    originally trained on Keras*, then converted to TensorFlow* protobuf format. For
    details, see paper <https://arxiv.org/abs/1708.02002>, repository <https://github.com/fizyr/keras-retinanet>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/fizyr/keras-retinanet/master/LICENSE
  name: retinanet-tf
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/retinanet-tf
  source: public
  task_type: detection
  tf_devices: CPU
rexnet-v1-x1.0:
  GFLOPs: '0.8325'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'ReXNet V1 x1.0 is network from Rank eXpansion Network (ReXNet) models
    family, derived from research to mitigate the representational bottleneck. It
    is image classification model pre-trained on ImageNet dataset.

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications  matching
    with those in the ImageNet database.

    For details see repository <https://github.com/clovaai/rexnet> and paper <https://arxiv.org/pdf/2007.00992.pdf>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/clovaai/rexnet/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: rexnet-v1-x1.0
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/rexnet-v1-x1.0
  source: public
  task_type: classification
rfcn-resnet101-coco-tf:
  Source framework: TensorFlow\*
  Type: Object detection
  coco_precision: 28.40%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: R-FCN ResNet-101 model, pre-trained on Common Objects in Context (COCO)
    <https://cocodataset.org/#home> dataset. Used for object detection. For details,
    see the paper <https://arxiv.org/abs/1605.06409>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  mAP: 45.02%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-image-info.json
  name: rfcn-resnet101-coco-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/rfcn-resnet101-coco-tf
  source: public
  task_type: detection
  tf_devices: CPU
robust-video-matting-mobilenetv3:
  Alpha GRAD: '4.44'
  Alpha MAD: '20.79'
  Alpha MSE: '15.1'
  Foreground MSE: '4.05'
  Source framework: PyTorch\*
  Type: Background_matting
  demo_apps:
  - background_subtraction_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  description: The "robust-video-matting-mobilenetv3" model is a robust high-resolution
    human video matting method that uses a recurrent architecture to exploit temporal
    information in videos and achieves significant improvements in temporal coherence
    and matting quality. This model is pre-trained in PyTorch* framework and converted
    to ONNX* format. More details provided in the paper <https://arxiv.org/abs/2108.11515>.
    Backbone is MobileNetV3. For details see the repository <https://github.com/PeterL1n/RobustVideoMatting>.
    For details regarding export to ONNX see the instruction <https://github.com/DmitriySidnev/RobustVideoMatting#export-to-onnx>.
  license: https://github.com/DmitriySidnev/RobustVideoMatting/blob/master/LICENSE
  name: robust-video-matting-mobilenetv3
  openvino_devices: ''
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/robust-video-matting-mobilenetv3
  source: public
  task_type: background_matting
shufflenet-v2-x1.0:
  GFLOPs: '0.2957'
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'Shufflenet V2 x1.0 is image classification model pre-trained on ImageNet
    dataset. This is PyTorch* implementation based on architecture described in paper
    "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design" <https://arxiv.org/abs/1807.11164>
    in TorchVision package (see here <https://github.com/pytorch/vision>).

    The model input is a blob that consists of a single image of "1, 3, 224, 224"
    in "RGB" order.

    The model output is typical object classifier for the 1000 different classifications
    matching with those in the ImageNet database.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/pytorch/vision/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: shufflenet-v2-x1.0
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/shufflenet-v2-x1.0
  source: public
  task_type: classification
single-human-pose-estimation-0001:
  AP(coco orig): 69.04%
  Source framework: PyTorch\*
  demo_apps:
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: Single human pose estimation model based on paper <https://arxiv.org/abs/1906.04104>.
  license: https://raw.githubusercontent.com/opencv/openvino_training_extensions/develop/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/single-human-pose-estimation-0001.json
  name: single-human-pose-estimation-0001
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/single-human-pose-estimation-0001
  source: public
  task_type: human_pose_estimation
single-image-super-resolution-1032:
  PSNR: 29.29 dB
  Source framework: PyTorch\*
  demo_apps:
  - image_processing_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_processing_demo/cpp
  description: Super resolution model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: single-image-super-resolution-1032
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/single-image-super-resolution-1032
  source: intel
  task_type: image_processing
single-image-super-resolution-1033:
  PSNR: 30.97 dB
  Source framework: PyTorch\*
  demo_apps:
  - image_processing_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_processing_demo/cpp
  description: Super resolution model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: single-image-super-resolution-1033
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/single-image-super-resolution-1033
  source: intel
  task_type: image_processing
smartlab-action-recognition-0001:
  Accuracy on the DSI1867: TODO
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: There are 3 models for smartlab action recogntion including two encoder
    models and one decoder model. These models are fine-tuned with smartlab dataset
    to predict actions and can classfy 3 types of action including "noise_action",
    "put_take" and "adjust_rider".
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-action-recognition-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-action-recognition-0001
  source: intel
  stages_order:
  - smartlab-action-recognition-0001-encoder-side
  - smartlab-action-recognition-0001-encoder-top
  - smartlab-action-recognition-0001-decoder
  task_type: action_recognition
smartlab-object-detection-0001:
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: Yolox based object detector with all 10 class (balance -  weights -  tweezers
    -  box -  battery -  tray -  ruler -  rider -  scale -  hand) for mythware top-view
    stream
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-object-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-object-detection-0001
  source: intel
  task_type: detection
smartlab-object-detection-0002:
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: Yolox based object detector with 3 moving-object class (weights -  tweezers
    -  battery) for Mythware top-view stream
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-object-detection-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-object-detection-0002
  source: intel
  task_type: detection
smartlab-object-detection-0003:
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: Yolox based object detector with all 10 class (balance -  weights -  tweezers
    -  box -  battery -  tray -  ruler -  rider -  scale -  hand) for Mythware front-view
    stream
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-object-detection-0003
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-object-detection-0003
  source: intel
  task_type: detection
smartlab-object-detection-0004:
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: Yolox based object detector with 3 moving-object class (weights -  tweezers
    -  battery) for Mythware front-view stream
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-object-detection-0004
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-object-detection-0004
  source: intel
  task_type: detection
smartlab-sequence-modelling-0001:
  GOPs: '0.11'
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: Image feature extraction network modified from mobilenet-v3 where origianl
    classifier layer is dropped. Input is rgb image and output is feature vector
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-sequence-modelling-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-sequence-modelling-0001
  source: intel
  task_type: action_recognition
smartlab-sequence-modelling-0002:
  GOPs: '0.048915'
  Source framework: PyTorch\*
  demo_apps:
  - smartlab_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/smartlab_demo/python
  description: online version of MS-TCN++(multi-stage temporal convolutional network)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: smartlab-sequence-modelling-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/smartlab-sequence-modelling-0002
  source: intel
  task_type: action_recognition
ssd-resnet34-1200-onnx:
  GFLOPs: '433.411'
  Source framework: PyTorch\*
  Type: Detection
  coco_precision: 20.73%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: The "ssd-resnet-34-1200-onnx" model is a multiscale SSD based on ResNet-34
    backbone network intended to perform object detection. The model has been trained
    from the Common Objects in Context (COCO) image dataset. This model is pre-trained
    in PyTorch* framework and converted to ONNX* format. For additional information
    refer to repository <https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection>.
  license: https://raw.githubusercontent.com/mlcommons/inference/master/LICENSE.md
  name: ssd-resnet34-1200-onnx
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ssd-resnet34-1200-onnx
  source: public
  task_type: detection
ssd_mobilenet_v1_coco:
  GFLOPs: '2.494'
  Source framework: TensorFlow\*
  Type: Detection
  coco_precision: 23.3212%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: The "ssd_mobilenet_v1_coco" model is a Single-Shot multibox Detection
    (SSD) <https://arxiv.org/abs/1801.04381> network intended to perform object detection.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  name: ssd_mobilenet_v1_coco
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ssd_mobilenet_v1_coco
  source: public
  task_type: detection
  tf_devices: CPU
ssd_mobilenet_v1_fpn_coco:
  GFLOPs: '123.309'
  Source framework: TensorFlow\*
  Type: Detection
  coco_precision: 35.5453%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: MobileNetV1 FPN is used for object detection. For details, see the
    paper <https://arxiv.org/abs/1807.03284>.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  name: ssd_mobilenet_v1_fpn_coco
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ssd_mobilenet_v1_fpn_coco
  source: public
  task_type: detection
  tf_devices: CPU
ssdlite_mobilenet_v2:
  GFLOPs: '1.525'
  Source framework: TensorFlow\*
  Type: Detection
  coco_precision: 24.2946%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - single_human_pose_estimation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/single_human_pose_estimation_demo/python
  description: 'The "ssdlite_mobilenet_v2" model is used for object detection. For
    details, see the paper <https://arxiv.org/abs/1801.04381>, MobileNetV2: Inverted
    Residuals and Linear Bottlenecks.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_91cl_bkgr.txt
  license: https://raw.githubusercontent.com/tensorflow/models/master/LICENSE
  name: ssdlite_mobilenet_v2
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ssdlite_mobilenet_v2
  source: public
  task_type: detection
  tf_devices: CPU
swin-tiny-patch4-window7-224:
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'The "swin-tiny-patch4-window7-224" model is a "tiny" version of the
    Swin Transformer image classification models pre-trained on ImageNet dataset.
    Swin Transformer is Hierarchical Vision Transformer whose representation is computed
    with shifted windows. Each patch is treated as a token with size of 4 and its
    feature is set as a concatenation of the raw pixel RGB values. The model has 7
    patches in each window. Stages of tiny version of model have 2, 2, 6, 2 layers
    respectively. Number of channels of the hidden layers in the first stage for tiny
    variant is 96.

    More details provided in the paper <https://arxiv.org/pdf/2103.14030.pdf> and
    repository <https://github.com/rwightman/pytorch-image-models>.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/imagenet_2012.txt
  license: https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/preproc-aspect-ratio.json
  name: swin-tiny-patch4-window7-224
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/swin-tiny-patch4-window7-224
  source: public
  task_type: classification
t2t-vit-14:
  Source framework: PyTorch\*
  Type: Classification
  demo_apps:
  - classification_demo
  - classification_benchmark_demo
  - classification_benchmark_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp_gapi
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/classification_benchmark_demo/cpp
  description: 'The "t2t-vit-14" model is a variant of the Tokens-To-Token Vision
    Transformer(T2T-ViT) pre-trained on ImageNet dataset for image classification
    task. T2T-ViT progressively tokenize the image to tokens and has an efficient
    backbone. T2T-ViT consists of two main components: 1) a layer-wise "Tokens-to-Token
    module" to model the local structure information of the image and reduce the length
    of tokens progressively; 2) an efficient "T2T-ViT backbone" to draw the global
    attention relation on tokens from the T2T module. The model has 14 transformer
    layers in T2T-ViT backbone with 384 hidden dimensions.

    More details provided in the paper <https://arxiv.org/abs/2101.11986> and repository
    <https://github.com/yitu-opensource/T2T-ViT>.'
  license: https://raw.githubusercontent.com/yitu-opensource/T2T-ViT/main/LICENSE
  name: t2t-vit-14
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/t2t-vit-14
  source: public
  task_type: classification
text-detection-0004:
  F-measure (Harmonic mean of precision and recall on ICDAR2015): 79.43%
  Source framework: TensorFlow\*
  demo_apps:
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: Detects oriented text.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-detection-0004
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-detection-0004
  source: intel
  task_type: detection
text-image-super-resolution-0001:
  PSNR: 20.318 dB
  Source framework: PyTorch\*
  demo_apps:
  - image_processing_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/image_processing_demo/cpp
  description: Super resolution model
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-image-super-resolution-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-image-super-resolution-0001
  source: intel
  task_type: image_processing
text-recognition-0012:
  Accuracy on the alphanumeric subset of ICDAR13: '0.8818'
  Source framework: TensorFlow\*
  Text location requirements: Tight aligned crop
  demo_apps:
  - text_detection_demo
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: 'Recognizes alphanumeric text. Architecture: VGG-like + BiLSTM as an
    encoder -  BiLSTM as a decoder.'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-recognition-0012
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-recognition-0012
  source: intel
  task_type: optical_character_recognition
text-recognition-0014:
  Source framework: PyTorch\*
  Text location requirements: Tight aligned crop
  demo_apps:
  - text_detection_demo
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: 'Recognizes alphanumeric text. Architecture: ResNeXt101 stage2 + BiLSTM
    as an encoder -  BiLSTM as a decoder.'
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-recognition-0014
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-recognition-0014
  source: intel
  task_type: optical_character_recognition
text-recognition-0015:
  Source framework: PyTorch\*
  Text location requirements: Tight aligned crop
  demo_apps:
  - text_detection_demo
  - text_detection_demo
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: This is a text-recognition composite model that recognizes scene text.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-recognition-0015
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-recognition-0015
  source: intel
  stages_order:
  - text-recognition-0015-encoder
  - text-recognition-0015-decoder
  task_type: optical_character_recognition
text-recognition-0016:
  Source framework: PyTorch\*
  Text location requirements: Tight aligned crop
  demo_apps:
  - text_detection_demo
  - text_detection_demo
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: This is a text-recognition composite model that recognizes scene text.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: text-recognition-0016
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-recognition-0016
  source: intel
  stages_order:
  - text-recognition-0016-encoder
  - text-recognition-0016-decoder
  task_type: optical_character_recognition
text-recognition-resnet-fc:
  Source framework: PyTorch\*
  Type: Scene Text Recognition
  demo_apps:
  - text_detection_demo
  - text_detection_demo
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: '"text-recognition-resnet-fc" is a simple and preformant scene text
    recognition model based on ResNet with Fully Connected text recognition head.
    Source implementation on a PyTorch* framework could be found here <https://github.com/Media-Smart/vedastr>.
    Model is able to recognize alphanumeric text.'
  license: https://raw.githubusercontent.com/Media-Smart/vedastr/0fd2a0bd7819ae4daa2a161501e9f1c2ac67e96a/LICENSE
  name: text-recognition-resnet-fc
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/text-recognition-resnet-fc
  source: public
  task_type: optical_character_recognition
text-spotting-0005:
  Source framework: PyTorch\*
  Word spotting hmean ICDAR2015, without a dictionary: 71.29%
  demo_apps:
  - text_spotting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_spotting_demo/python
  description: This is a text spotting composite model that simultaneously detects
    and recognizes text. The model detects symbol sequences separated by space and
    performs recognition without a dictionary. The model is built on top of the Mask-RCNN
    framework with additional attention-based text recognition head.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  module: custom_evaluators.text_spotting_evaluator.TextSpottingEvaluator
  name: text-spotting-0005
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/text-spotting-0005
  source: intel
  stages_order:
  - text-spotting-0005-detector
  - text-spotting-0005-recognizer-encoder
  - text-spotting-0005-recognizer-decoder
  task_type: optical_character_recognition
time-series-forecasting-electricity-0001:
  GOps: '0.40'
  Normalized Quantile Loss (P50): '0.056'
  Normalized Quantile Loss (P90): '0.028'
  Source framework: PyTorch\*
  demo_apps:
  - time_series_forecasting_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/time_series_forecasting_demo/python
  description: Time Series Forecasting Model based on Temporal Fusion Transformer
    trained on Electricity dataset.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: time-series-forecasting-electricity-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/time-series-forecasting-electricity-0001
  source: intel
  task_type: time_series
ultra-lightweight-face-detection-rfb-320:
  GFLOPs: '0.2106'
  Source framework: PyTorch\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'Ultra-lightweight Face Detection RFB 320 is a version of the lightweight
    face detection model with the modified RFB(Receptive Field Block) module. The
    model designed for edge computing devices and pre-trained on the WIDER FACE <http://shuoyang1213.me/WIDERFACE/>
    dataset with 320x240 input resolutions.

    For details see repository <https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB>.'
  license: https://raw.githubusercontent.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/master/LICENSE
  mAP: 84.78%
  name: ultra-lightweight-face-detection-rfb-320
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ultra-lightweight-face-detection-rfb-320
  source: public
  task_type: detection
ultra-lightweight-face-detection-slim-320:
  GFLOPs: '0.1724'
  Source framework: PyTorch\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'Ultra-lightweight Face Detection slim 320 is a version of the lightweight
    face detection model with network backbone simplification. The model designed
    for edge computing devices and pre-trained on the WIDER FACE <http://shuoyang1213.me/WIDERFACE/>
    dataset with 320x240 input resolutions.

    For details see repository <https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB>.'
  license: https://raw.githubusercontent.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/master/LICENSE
  mAP: 83.32%
  name: ultra-lightweight-face-detection-slim-320
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/ultra-lightweight-face-detection-slim-320
  source: public
  task_type: detection
unet-camvid-onnx-0001:
  Source framework: PyTorch\*
  demo_apps:
  - segmentation_demo
  - segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/segmentation_demo/python
  description: U-Net trained on CamVid in PyTorch (71.95% mIoU)
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  mIoU: 71.95%
  name: unet-camvid-onnx-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/unet-camvid-onnx-0001
  source: intel
  task_type: semantic_segmentation
vehicle-attributes-recognition-barrier-0039:
  ? ''
  : car
  '**black**': '0.85'
  '**blue**': '**79.53**'
  '**bus**': '7.94'
  '**car**': '**98.26**'
  '**gray**': '2.53'
  '**green**': '3.79'
  '**red**': '0.89'
  '**track**': '1.71'
  '**van**': '3.72'
  '**white**': '1.45'
  '**yellow**': '0'
  Car pose: Front facing cars
  Min object width: 72 pixels
  Occlusion coverage: <50%
  Source framework: Caffe\*
  Supported colors: White, gray, yellow, red, green, blue, black
  Supported types: Car, bus, truck, van
  demo_apps:
  - security_barrier_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  description: Vehicle attributes recognition with modified ResNet10 backbone
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-attributes-recognition-barrier-0039.json
  name: vehicle-attributes-recognition-barrier-0039
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-attributes-recognition-barrier-0039
  source: intel
  task_type: object_attributes
vehicle-attributes-recognition-barrier-0042:
  ':--------:': ':----------:'
  ':-----:': ':--------:'
  Car pose: Front facing cars
  Color: Accuracy
  Min object width: 72 pixels
  Occlusion coverage: <50%
  Source framework: PyTorch\*
  Supported colors: White, gray, yellow, red, green, blue, black
  Supported types: Car, van, truck, bus
  Type: Accuracy
  black: 96.84%
  blue: 82.49%
  bus: 68.57%
  car: 97.44%
  demo_apps:
  - security_barrier_camera_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  description: Vehicle attributes recognition with modified ResNet18 backbone
  gray: 77.47%
  green: 81.82%
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-attributes-recognition-barrier-0042.json
  name: vehicle-attributes-recognition-barrier-0042
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-attributes-recognition-barrier-0042
  red: 94.65%
  source: intel
  task_type: object_attributes
  truck: 96.95%
  van: 86.41%
  white: 84.20%
  yellow: 61.50%
vehicle-detection-0200:
  AP @ [ IoU=0.50:0.95 ]: 0.254 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: MobileNetV2-SSD with two heads and clustered priors for 256x256 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-detection-0200.json
  name: vehicle-detection-0200
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-detection-0200
  source: intel
  task_type: detection
vehicle-detection-0201:
  AP @ [ IoU=0.50:0.95 ]: 0.322 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: MobileNetV2-SSD with two heads and clustered priors for 384x384 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-detection-0201.json
  name: vehicle-detection-0201
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-detection-0201
  source: intel
  task_type: detection
vehicle-detection-0202:
  AP @ [ IoU=0.50:0.95 ]: 0.363 (internal test set)
  Source framework: PyTorch\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: MobileNetV2-SSD with two heads and clustered priors for 512x512 resolution.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-detection-0202.json
  name: vehicle-detection-0202
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-detection-0202
  source: intel
  task_type: detection
vehicle-detection-adas-0002:
  Average Precision (AP): 90.6%
  Max objects to detect: '200'
  Source framework: Caffe\*
  Target vehicle size: 40 x 30 pixels on Full HD image
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: Vehicle detector based on SSD + MobileNet with reduced number of channels
    and depthwise head.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-detection-adas-0002.json
  name: vehicle-detection-adas-0002
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-detection-adas-0002
  source: intel
  task_type: detection
vehicle-license-plate-detection-barrier-0106:
  AP plates: 99.42%
  AP vehicles: 99.88%
  Car pose: Front facing cars
  Max objects to detect: '200'
  Mean Average Precision (mAP): 99.65%
  Min plate width: 96 pixels
  Source framework: TensorFlow\*
  demo_apps:
  - security_barrier_camera_demo
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Multiclass (vehicle -  license plates) detector based on MobileNetV2+SSD
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/vehicle-license-plate-detection-barrier-0106.json
  name: vehicle-license-plate-detection-barrier-0106
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/vehicle-license-plate-detection-barrier-0106
  source: intel
  task_type: detection
vehicle-license-plate-detection-barrier-0123:
  AP plates: 99.13%
  AP vehicles: 99.90%
  Car pose: Front facing cars
  Max objects to detect: '200'
  Mean Average Precision (mAP): 99.52%
  Min plate width: 96 pixels
  Source framework: TensorFlow*
  demo_apps:
  - security_barrier_camera_demo
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/security_barrier_camera_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: This is a MobileNetV2 + SSD-based vehicle and (Chinese) license plate
    detector for the "Barrier" use case.
  license: https://raw.githubusercontent.com/opencv/training_toolbox_tensorflow/develop/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/vehicle-license-plate-detection-barrier-0123.json
  name: vehicle-license-plate-detection-barrier-0123
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/vehicle-license-plate-detection-barrier-0123
  source: public
  task_type: detection
  tf_devices: CPU
vehicle-reid-0001:
  Camera location: All traffic cameras
  Occlusion coverage: <50%
  Source framework: PyTorch\*
  Support of occluded vehicles: 'YES'
  VeRi-776\* mAP: 85.15 %
  VeRi-776\* rank-1: 96.31 %
  demo_apps:
  - multi_camera_multi_target_tracking_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_camera_multi_target_tracking_demo/python
  description: This is a vehicle reidentification model for a general scenario. It
    uses a whole car body image as an input and outputs an embedding vector to match
    a pair of images by the cosine distance. The model is based on the OmniScaleNet
    backbone developed for fast inference. A single reidentification head from the
    1/16 scale feature map outputs an embedding vector of 512 floats.
  license: https://raw.githubusercontent.com/sovrasov/deep-person-reid/vehicle_reid/LICENSE
  name: vehicle-reid-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/vehicle-reid-0001
  source: public
  task_type: object_attributes
vitstr-small-patch16-224:
  Source framework: PyTorch\*
  Type: Scene Text Recognition
  demo_apps:
  - text_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_detection_demo/cpp
  description: 'The "vitstr-small-patch16-224" model is "small" version of the ViTSTR
    models. ViTSTR is a simple single-stage model that uses a pre-trained Vision Transformer
    (ViT) to perform Scene Text Recognition (ViTSTR). Small version of model has an
    embedding size of 384 and number of heads of 6. Model is able to recognize alphanumeric
    case sensitive text and special characters.

    More details provided in the paper <https://arxiv.org/abs/2105.08582> and repository
    <https://github.com/roatienza/deep-text-recognition-benchmark>.'
  license: https://raw.githubusercontent.com/roatienza/deep-text-recognition-benchmark/master/LICENSE.md
  name: vitstr-small-patch16-224
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/vitstr-small-patch16-224
  source: public
  task_type: optical_character_recognition
wav2vec2-base:
  GFLOPs: '26.843'
  Source framework: PyTorch\*
  Type: Speech recognition
  WER @ Librispeech test-clean: 3.39%
  demo_apps:
  - speech_recognition_wav2vec_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/speech_recognition_wav2vec_demo/python
  license: https://raw.githubusercontent.com/pytorch/fairseq/master/LICENSE
  name: wav2vec2-base
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/wav2vec2-base
  source: public
  task_type: named_entity_recognition
wavernn:
  GOPs: '0.37'
  GOps: '0.06'
  Source framework: PyTorch\*
  demo_apps:
  - text_to_speech_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/text_to_speech_demo/python
  license: https://github.com/fatchord/WaveRNN/blob/master/LICENSE.txt
  name: wavernn
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/wavernn
  source: public
  stages_order:
  - wavernn-upsampler
  - wavernn-rnn
  task_type: text_to_speech
weld-porosity-detection-0001:
  Clip classification accuracy: 97.14% (internal test set)
  Source framework: PyTorch\*
  Temporal smoothing window size: '16'
  demo_apps:
  - action_recognition_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/action_recognition_demo/python
  description: ResNet18 based network for porosity weld recognition.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/intel/weld-porosity-detection-0001.json
  name: weld-porosity-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/weld-porosity-detection-0001
  source: intel
  task_type: action_recognition
yolact-resnet50-fpn-pytorch:
  Source framework: PyTorch\*
  Type: Instance segmentation
  '`AP@boxes`': 30.69%
  '`AP@masks`': 28.00%
  demo_apps:
  - background_subtraction_demo
  - instance_segmentation_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/background_subtraction_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/instance_segmentation_demo/python
  description: 'YOLACT ResNet 50 is a simple, fully convolutional model for real-time
    instance segmentation described in "YOLACT: Real-time Instance Segmentation" paper
    <https://arxiv.org/abs/1904.02689>. Model pre-trained in Pytorch* on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset. For details, see the
    repository <https://github.com/dbolya/yolact>.'
  license: https://raw.githubusercontent.com/dbolya/yolact/master/LICENSE
  name: yolact-resnet50-fpn-pytorch
  openvino_devices: CPU, GPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolact-resnet50-fpn-pytorch
  source: public
  task_type: instance_segmentation
yolo-v1-tiny-tf:
  GFLOPs: '6.988'
  Source framework: TensorFlow.js\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v1 Tiny is a real-time object detection model from TensorFlow.js*
    framework. This model was pre-trained on VOC dataset with 20 classes.
  license: https://raw.githubusercontent.com/shaqian/tfjs-yolo/master/LICENSE
  mAP: 54.79%
  name: yolo-v1-tiny-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v1-tiny-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v2-ava-0001:
  Mean Average Precision (mAP): 63.9%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v2 trained on VOC dataset by AVA team with zero sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-ava-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-ava-0001
  source: intel
  task_type: detection
yolo-v2-ava-sparse-35-0001:
  Mean Average Precision (mAP): 63.71%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v2 trained on VOC dataset by AVA team with 0.35 sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-ava-sparse-35-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-ava-sparse-35-0001
  source: intel
  task_type: detection
yolo-v2-ava-sparse-70-0001:
  Mean Average Precision (mAP): 62.9%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v2 trained on VOC dataset by AVA team with 0.7 sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-ava-sparse-70-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-ava-sparse-70-0001
  source: intel
  task_type: detection
yolo-v2-tf:
  GFLOPs: '63.03'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v2 is a real-time object detection model implemented with Keras*
    from this repository <https://github.com/david8862/keras-YOLOv3-model-set> and
    converted to TensorFlow* framework. This model was pre-trained on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset with 80 classes.
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 53.15%
  name: yolo-v2-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v2-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v2-tiny-ava-0001:
  Mean Average Precision (mAP): 35.37%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO Tiny v2 trained on VOC dataset by AVA team with zero sparsity
    level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-tiny-ava-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-tiny-ava-0001
  source: intel
  task_type: detection
yolo-v2-tiny-ava-sparse-30-0001:
  Mean Average Precision (mAP): 36.33%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO Tiny v2 trained on VOC dataset by AVA team with 0.3 sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-tiny-ava-sparse-30-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-tiny-ava-sparse-30-0001
  source: intel
  task_type: detection
yolo-v2-tiny-ava-sparse-60-0001:
  Mean Average Precision (mAP): 35.32%
  Source framework: TensorFlow\*
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO Tiny v2 trained on VOC dataset by AVA team with 0.6 sparsity level.
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  name: yolo-v2-tiny-ava-sparse-60-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-tiny-ava-sparse-60-0001
  source: intel
  task_type: detection
yolo-v2-tiny-tf:
  GFLOPs: '5.424'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO v2 Tiny is a real-time object detection model implemented with
    Keras* from this repository <https://github.com/david8862/keras-YOLOv3-model-set>
    and converted to TensorFlow* framework. This model was pre-trained on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset with 80 classes.
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 27.34%
  name: yolo-v2-tiny-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v2-tiny-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v2-tiny-vehicle-detection-0001:
  GFLOPs: '5.424'
  Source framework: Keras\*
  Type: Detection
  coco\_precision: 94.97%
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  description: YOLO Tiny v2 (https://arxiv.org/abs/1612.08242) fine-tuned for vehicles
    detection
  license: https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE
  mAP: 88.64%
  name: yolo-v2-tiny-vehicle-detection-0001
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/intel/yolo-v2-tiny-vehicle-detection-0001
  source: intel
  task_type: detection
yolo-v3-onnx:
  GFLOPs: '65.998'
  Source framework: ONNX\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: YOLO v3 is a real-time object detection model in ONNX* format from
    the repository <https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov3>
    which is converted from Keras* model repository <https://github.com/qqwweee/keras-yolo3>
    using keras2onnx converter <https://github.com/onnx/keras-onnx>. This model was
    pre-trained on Common Objects in Context <COCO> <https://cocodataset.org/#home>
    dataset with 80 classes.
  license: https://raw.githubusercontent.com/onnx/models/master/LICENSE
  mAP: 48.30%
  name: yolo-v3-onnx
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v3-onnx
  source: public
  task_type: detection
yolo-v3-tf:
  GFLOPs: '65.984'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - multi_channel_object_detection_demo_yolov3
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_object_detection_demo_yolov3/cpp
  description: YOLO v3 is a real-time object detection model implemented with Keras*
    from this repository <https://github.com/david8862/keras-YOLOv3-model-set> and
    converted to TensorFlow* framework. This model was pre-trained on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset with 80 classes.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 62.27%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v3-tf.json
  name: yolo-v3-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v3-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v3-tiny-onnx:
  GFLOPs: '5.582'
  Source framework: ONNX\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: Tiny YOLO v3 is a smaller version of real-time object detection YOLO
    v3 model in ONNX* format from the repository <https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov3>
    which is converted from Keras* model repository <https://github.com/qqwweee/keras-yolo3>
    using keras2onnx converter <https://github.com/onnx/keras-onnx>. This model was
    pre-trained on Common Objects in Context <COCO> <https://cocodataset.org/#home>
    dataset with 80 classes.
  license: https://raw.githubusercontent.com/onnx/models/master/LICENSE
  mAP: 17.07%
  name: yolo-v3-tiny-onnx
  openvino_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v3-tiny-onnx
  source: public
  task_type: detection
yolo-v3-tiny-tf:
  GFLOPs: '5.582'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  - pedestrian_tracker_demo
  - multi_channel_object_detection_demo_yolov3
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/pedestrian_tracker_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/multi_channel_object_detection_demo_yolov3/cpp
  description: YOLO v3 Tiny is a real-time object detection model implemented with
    Keras* from this repository <https://github.com/david8862/keras-YOLOv3-model-set>
    and converted to TensorFlow* framework. This model was pre-trained on Common Objects
    in Context (COCO) <https://cocodataset.org/#home> dataset with 80 classes.
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 35.9%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v3-tiny-tf.json
  name: yolo-v3-tiny-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v3-tiny-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v4-tf:
  GFLOPs: '129.5567'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'YOLO v4 is a real-time object detection model based on "YOLOv4: Optimal
    Speed and Accuracy of Object Detection" <https://arxiv.org/abs/2004.10934> paper.
    It was implemented in Keras* framework and converted to TensorFlow* framework.
    For details see repository <https://github.com/david8862/keras-YOLOv3-model-set>.
    This model was pre-trained on Common Objects in Context (COCO) <https://cocodataset.org/#home>
    dataset with 80 classes.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 71.23%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v4-tf.json
  name: yolo-v4-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v4-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolo-v4-tiny-tf:
  GFLOPs: '6.9289'
  Source framework: Keras\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'YOLO v4 Tiny is a real-time object detection model based on "YOLOv4:
    Optimal Speed and Accuracy of Object Detection" <https://arxiv.org/abs/2004.10934>
    paper. It was implemented in Keras* framework and converted to TensorFlow* framework.
    For details see repository <https://github.com/david8862/keras-YOLOv3-model-set>.
    This model was pre-trained on Common Objects in Context (COCO) <https://cocodataset.org/#home>
    dataset with 80 classes.'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  license: https://raw.githubusercontent.com/david8862/keras-YOLOv3-model-set/master/LICENSE
  mAP: 40.37%
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/public/yolo-v4-tiny-tf.json
  name: yolo-v4-tiny-tf
  openvino_devices: CPU, GPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolo-v4-tiny-tf
  source: public
  task_type: detection
  tf_devices: CPU
yolof:
  GFLOPs: '175.37942'
  Source framework: PyTorch\*
  Type: Detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: YOLOF is a simple, fast, and efficient object detector without FPN.
    Model based on "You Only Look One-level Feature" <https://arxiv.org/abs/2103.09460>
    paper. It was implemented in PyTorch* framework. Model used "DarkNet-53" with
    Cross Stage Partial blocks as backbone. For details see repository <https://github.com/megvii-model/YOLOF>.
    This model was pre-trained on Common Objects in Context (COCO) <https://cocodataset.org/#home>
    dataset with 80 classes
  license: https://raw.githubusercontent.com/megvii-model/YOLOF/main/LICENSE
  mAP: 60.69%
  name: yolof
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolof
  source: public
  task_type: detection
yolox-tiny:
  GFLOPs: '6.4813'
  Source framework: PyTorch\*
  Type: Object detection
  demo_apps:
  - object_detection_demo
  - object_detection_demo
  demo_urls:
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/cpp
  - https://github.com/openvinotoolkit/open_model_zoo/tree/master//demos/object_detection_demo/python
  description: 'The "yolox-tiny" is a tiny version of YOLOX models family for object
    detection tasks. YOLOX is an anchor-free version of YOLO, with a simpler design
    but better performance.This model was pre-trained on Common Objects in Context
    <COCO> <https://cocodataset.org/#home> dataset with 80 classes.

    More details provided in the paper <https://arxiv.org/abs/2107.08430> and repository
    <https://github.com/Megvii-BaseDetection/YOLOX>.'
  license: https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/LICENSE
  mAP: 47.85%
  name: yolox-tiny
  openvino_devices: CPU
  pytorch_devices: CPU
  readme: https://github.com/openvinotoolkit/open_model_zoo/tree/master//models/public/yolox-tiny
  source: public
  task_type: detection

  # public models
mobilenetv2-7:
  source: public
  format: onnx
  task_type: classification
  readme: https://github.com/onnx/models/tree/main/validated/vision/classification/mobilenet
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/onnx/mobilenetv2-7.json
emotion-ferplus-8:
  source: public
  format: onnx
  task_type: classification
  readme: https://github.com/onnx/models/tree/main/validated/vision/body_analysis/emotion_ferplus
  model-proc: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/gstreamer/model_proc/onnx/emotion-ferplus-8.json

# TorchVision models
torchvision.models.detection.ssdlite320_mobilenet_v3_large:
  source: torchvision
  format: pytorch
  task_type: detection
  readme: https://pytorch.org/vision/main/models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html
  GFLOPs: '0.583'
  labels-file: https://github.com/open-edge-platform/edge-ai-libraries/tree/main/libraries/dl-streamer/samples/labels/coco_80cl.txt
  dlstreamer_support: gva, gst_openvino, gst_pytorch, ffmpeg_openvino
  dlstreamer_gva: gvadetect model=ssdlite320_mobilenet_v3_large.xml device=CPU
  dlstreamer_gst_openvino: object_detect model=ssdlite320_mobilenet_v3_large.xml device=CPU
  dlstreamer_gst_pytorch: object_detect model=torchvision.models.detection.ssdlite320_mobilenet_v3_large device=CPU
  dlstreamer_ffmpeg_openvino: $DLSTREAMER_DIR/samples/ffmpeg_openvino/cpp/decode_resize_inference/build_and_run.sh VIDEO_FILE ssdlite320_mobilenet_v3_large.xml

